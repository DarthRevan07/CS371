{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3RprH1CDIb_",
        "outputId": "265a8034-ec61-4709-c8ca-625efd0d905e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.2-py3-none-any.whl (1.1 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.1 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.2\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich) (0.1.2)\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.4.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.25.2)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (2.2.1+cu121)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.10/dist-packages (from opacus) (1.11.4)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from opacus) (3.3.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=2.0->opacus)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0->opacus) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.0->opacus)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0->opacus) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.0->opacus) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, opacus\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 opacus-1.4.1\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.45.0-py2.py3-none-any.whl (267 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.1/267.1 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.43 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.45.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.6\n",
            "Collecting dask_jobqueue\n",
            "  Downloading dask_jobqueue-0.8.5-py2.py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dask>=2022.02.0 in /usr/local/lib/python3.10/dist-packages (from dask_jobqueue) (2023.8.1)\n",
            "Requirement already satisfied: distributed>=2022.02.0 in /usr/local/lib/python3.10/dist-packages (from dask_jobqueue) (2023.8.1)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (8.1.7)\n",
            "Requirement already satisfied: cloudpickle>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (2.2.1)\n",
            "Requirement already satisfied: fsspec>=2021.09.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (24.0)\n",
            "Requirement already satisfied: partd>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (1.4.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (6.0.1)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.13.0 in /usr/local/lib/python3.10/dist-packages (from dask>=2022.02.0->dask_jobqueue) (7.1.0)\n",
            "Requirement already satisfied: jinja2>=2.10.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (3.1.3)\n",
            "Requirement already satisfied: locket>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (1.0.0)\n",
            "Requirement already satisfied: msgpack>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.7.2 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (5.9.5)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (2.4.0)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (3.0.0)\n",
            "Requirement already satisfied: tornado>=6.0.4 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (6.3.3)\n",
            "Requirement already satisfied: urllib3>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (2.0.7)\n",
            "Requirement already satisfied: zict>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from distributed>=2022.02.0->dask_jobqueue) (3.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata>=4.13.0->dask>=2022.02.0->dask_jobqueue) (3.18.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2>=2.10.3->distributed>=2022.02.0->dask_jobqueue) (2.1.5)\n",
            "Installing collected packages: dask_jobqueue\n",
            "Successfully installed dask_jobqueue-0.8.5\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.3.2-py3-none-any.whl (841 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m841.5/841.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.25.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (24.0)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.2.1+cu121)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.11.2-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (67.7.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.11.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.13.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->torchmetrics) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->torchmetrics) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->torchmetrics) (1.3.0)\n",
            "Installing collected packages: lightning-utilities, torchmetrics\n",
            "Successfully installed lightning-utilities-0.11.2 torchmetrics-1.3.2\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install --upgrade rich\n",
        "!pip install opacus\n",
        "!pip install wandb\n",
        "!pip install dask_jobqueue\n",
        "!pip install torchmetrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfH7944DCuck"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4i3iZidRXGaT",
        "outputId": "262aa265-c932-4313-fe4b-aa719b6cfc89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting class_resolver\n",
            "  Downloading class_resolver-0.4.3-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: class_resolver\n",
            "Successfully installed class_resolver-0.4.3\n"
          ]
        }
      ],
      "source": [
        "!pip install class_resolver"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_fSnvu5MEfI"
      },
      "source": [
        "Writing a class to download and process the Facebook dataset, which is seemingly the smallest sized-dataset upon which this analysis was performed :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63nai1wMLqIo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import ssl\n",
        "import torch\n",
        "import pandas as pd\n",
        "from scipy.io import loadmat\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from torch_geometric.utils import subgraph, from_scipy_sparse_matrix\n",
        "from torch_geometric.data import Data, InMemoryDataset, download_url\n",
        "\n",
        "\n",
        "class Facebook(InMemoryDataset):\n",
        "    url = 'https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100/'\n",
        "    targets = ['status', 'gender', 'major', 'minor', 'housing', 'year']\n",
        "    available_datasets = [\n",
        "        \"Villanova62\", \"UCLA26\", \"Tennessee95\", \"NYU9\", \"Carnegie49\", \"GWU54\", \"USF51\", \"Vanderbilt48\",\n",
        "        \"USC35\", \"Rutgers89\", \"UConn91\", \"MIT8\", \"USFCA72\", \"UChicago30\", \"UIllinois20\", \"UC61\", \"Cal65\",\n",
        "        \"Yale4\", \"Northeastern19\", \"Dartmouth6\", \"Vermont70\", \"Northwestern25\", \"William77\", \"Harvard1\",\n",
        "        \"Princeton12\", \"UC64\", \"Middlebury45\", \"Haverford76\", \"Bingham82\", \"UNC28\", \"Berkeley13\", \"Rochester38\",\n",
        "        \"Swarthmore42\", \"Virginia63\", \"WashU32\", \"Columbia2\", \"NotreDame57\", \"Bucknell39\", \"UVA16\", \"Maine59\",\n",
        "        \"MU78\", \"Simmons81\", \"MSU24\", \"Colgate88\", \"Temple83\", \"Cornell5\", \"Indiana69\", \"Oklahoma97\", \"Michigan23\",\n",
        "        \"BU10\", \"Brown11\", \"Auburn71\", \"FSU53\", \"UGA50\", \"UCF52\", \"Howard90\", \"UCSD34\", \"Vassar85\", \"Tufts18\",\n",
        "        \"UPenn7\", \"Baylor93\", \"UMass92\", \"Bowdoin47\", \"Maryland58\", \"Penn94\", \"Wesleyan43\", \"UC33\",\n",
        "        \"Rice31\", \"UCSC68\", \"Smith60\", \"Caltech36\", \"Hamilton46\", \"Oberlin44\", \"American75\", \"Mich67\",\n",
        "        \"Mississippi66\", \"Williams40\", \"UCSB37\", \"Amherst41\", \"Duke14\", \"Pepperdine86\", \"Wake73\", \"Lehigh96\",\n",
        "        \"Reed98\", \"Tulane29\", \"Texas84\", \"Wellesley22\", \"JMU79\", \"Santa74\", \"Wisconsin87\", \"Stanford3\",\n",
        "        \"Texas80\", \"UF21\", \"JohnsHopkins55\", \"Syracuse56\", \"BC17\", \"Georgetown15\", \"Trinity100\", \"Brandeis99\", \"Emory27\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root: str, name: str, target='status', transform=None, pre_transform=None):\n",
        "        self.name = name\n",
        "        self.target = target\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self) -> str:\n",
        "        return os.path.join(self.root, self.name, 'raw')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> str:\n",
        "        return self.name + '.mat'\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self) -> str:\n",
        "        return os.path.join(self.root, self.name, 'processed')\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        return f'data-{self.name}-{self.target}.pt'\n",
        "\n",
        "    def download(self):\n",
        "        context = ssl._create_default_https_context\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        download_url(f'{self.url}/{self.raw_file_names}', self.raw_dir)\n",
        "        ssl._create_default_https_context = context\n",
        "\n",
        "    def process(self):\n",
        "        mat = loadmat(os.path.join(self.raw_dir, self.raw_file_names))\n",
        "        features = pd.DataFrame(mat['local_info'][:, :-1], columns=self.targets)\n",
        "        y = torch.from_numpy(LabelEncoder().fit_transform(features[self.target]))\n",
        "        if 0 in features[self.target].values:\n",
        "            y = y - 1\n",
        "\n",
        "        x = features.drop(columns=self.target).replace({0: None})\n",
        "        x = torch.tensor(pd.get_dummies(x).values, dtype=torch.float)\n",
        "        edge_index = from_scipy_sparse_matrix(mat['A'])[0]\n",
        "\n",
        "        # removed unlabled nodes\n",
        "        subset = y >= 0\n",
        "        edge_index, _ = subgraph(subset, edge_index, relabel_nodes=True, num_nodes=len(y))\n",
        "        x = x[subset]\n",
        "        y = y[subset]\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, y=y)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'Facebook-{self.name}()'\n",
        "\n",
        "\n",
        "class FB100(InMemoryDataset):\n",
        "    url = 'https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100/'\n",
        "    targets = ['status', 'gender', 'major', 'minor', 'housing', 'year']\n",
        "    available_datasets = [\n",
        "        \"Villanova62\", \"UCLA26\", \"Tennessee95\", \"NYU9\", \"Carnegie49\", \"GWU54\", \"USF51\", \"Vanderbilt48\",\n",
        "        \"USC35\", \"Rutgers89\", \"UConn91\", \"MIT8\", \"USFCA72\", \"UChicago30\", \"UIllinois20\", \"UC61\", \"Cal65\",\n",
        "        \"Yale4\", \"Northeastern19\", \"Dartmouth6\", \"Vermont70\", \"Northwestern25\", \"William77\", \"Harvard1\",\n",
        "        \"Princeton12\", \"UC64\", \"Middlebury45\", \"Haverford76\", \"Bingham82\", \"UNC28\", \"Berkeley13\", \"Rochester38\",\n",
        "        \"Swarthmore42\", \"Virginia63\", \"WashU32\", \"Columbia2\", \"NotreDame57\", \"Bucknell39\", \"UVA16\", \"Maine59\",\n",
        "        \"MU78\", \"Simmons81\", \"MSU24\", \"Colgate88\", \"Temple83\", \"Cornell5\", \"Indiana69\", \"Oklahoma97\", \"Michigan23\",\n",
        "        \"BU10\", \"Brown11\", \"Auburn71\", \"FSU53\", \"UGA50\", \"UCF52\", \"Howard90\", \"UCSD34\", \"Vassar85\", \"Tufts18\",\n",
        "        \"UPenn7\", \"Baylor93\", \"UMass92\", \"Bowdoin47\", \"Maryland58\", \"Penn94\", \"Wesleyan43\", \"UC33\",\n",
        "        \"Rice31\", \"UCSC68\", \"Smith60\", \"Caltech36\", \"Hamilton46\", \"Oberlin44\", \"American75\", \"Mich67\",\n",
        "        \"Mississippi66\", \"Williams40\", \"UCSB37\", \"Amherst41\", \"Duke14\", \"Pepperdine86\", \"Wake73\", \"Lehigh96\",\n",
        "        \"Reed98\", \"Tulane29\", \"Texas84\", \"Wellesley22\", \"JMU79\", \"Santa74\", \"Wisconsin87\", \"Stanford3\",\n",
        "        \"Texas80\", \"UF21\", \"JohnsHopkins55\", \"Syracuse56\", \"BC17\", \"Georgetown15\", \"Trinity100\", \"Brandeis99\", \"Emory27\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, root: str, target='status', transform=None, pre_transform=None):\n",
        "        self.target = target\n",
        "        super().__init__(root, transform, pre_transform)\n",
        "        self.data, self.slices = torch.load(self.processed_paths[0])\n",
        "\n",
        "    @property\n",
        "    def raw_dir(self) -> str:\n",
        "        return os.path.join(self.root, 'raw')\n",
        "\n",
        "    @property\n",
        "    def raw_file_names(self) -> list[str]:\n",
        "        return [f'{name}.mat' for name in self.available_datasets]\n",
        "\n",
        "    @property\n",
        "    def processed_dir(self) -> str:\n",
        "        return os.path.join(self.root, 'processed')\n",
        "\n",
        "    @property\n",
        "    def processed_file_names(self) -> str:\n",
        "        return f'data-{self.target}.pt'\n",
        "\n",
        "    def download(self):\n",
        "        context = ssl._create_default_https_context\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "        for name in self.raw_file_names:\n",
        "            download_url(f'{self.url}/{name}', self.raw_dir)\n",
        "        ssl._create_default_https_context = context\n",
        "\n",
        "    def process(self):\n",
        "        features = pd.DataFrame()\n",
        "        edge_index = torch.tensor([], dtype=torch.long)\n",
        "\n",
        "        for i, raw_file_name in enumerate(self.raw_file_names):\n",
        "            mat = loadmat(os.path.join(self.raw_dir, raw_file_name))\n",
        "            mat_features = pd.DataFrame(mat['local_info'][:, :-1], columns=self.targets)\n",
        "            mat_edge_index = from_scipy_sparse_matrix(mat['A'])[0] + len(features)\n",
        "            if i < 75:\n",
        "                mat_features['split'] = 'train'\n",
        "            elif i < 85:\n",
        "                mat_features['split'] = 'val'\n",
        "            else:\n",
        "                mat_features['split'] = 'test'\n",
        "            features = pd.concat([features, mat_features], ignore_index=True)\n",
        "            edge_index = torch.cat([edge_index, mat_edge_index], dim=1)\n",
        "\n",
        "        y = torch.from_numpy(LabelEncoder().fit_transform(features[self.target]))\n",
        "        if 0 in features[self.target].values:\n",
        "            y = y - 1\n",
        "\n",
        "        x = features.drop(columns=[self.target, 'minor', 'housing', 'split']).replace({0: None})\n",
        "        x = torch.tensor(pd.get_dummies(x).values, dtype=torch.float)\n",
        "\n",
        "        train_mask = torch.from_numpy((features['split'] == 'train').values)\n",
        "        val_mask = torch.from_numpy((features['split'] == 'val').values)\n",
        "        test_mask = torch.from_numpy((features['split'] == 'test').values)\n",
        "\n",
        "        # removed unlabled nodes\n",
        "        subset = y >= 0\n",
        "        edge_index, _ = subgraph(subset, edge_index, relabel_nodes=True, num_nodes=len(y))\n",
        "        x = x[subset]\n",
        "        y = y[subset]\n",
        "        train_mask = train_mask[subset]\n",
        "        val_mask = val_mask[subset]\n",
        "        test_mask = test_mask[subset]\n",
        "\n",
        "        data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask, val_mask=val_mask, test_mask=test_mask)\n",
        "\n",
        "        if self.pre_transform is not None:\n",
        "            data = self.pre_transform(data)\n",
        "\n",
        "        torch.save(self.collate([data]), self.processed_paths[0])\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f'Facebook-100()'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJMjLAMTewTT"
      },
      "source": [
        "### Reddit dataset is a pre-loaded dataset in `PyTorch Geometric`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL5H7DhZMsrq"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.datasets import Reddit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7q40jnRMtVD"
      },
      "source": [
        "Adding some helper functions, as well as a Console to push the extracted data entities onto,\n",
        "\n",
        "* In the following section, we define a helper function that converts dictionary data structure to a table format for viewing purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cV4m75eMMea_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from typing import Callable, Optional\n",
        "import numpy as np\n",
        "from numpy.typing import ArrayLike, NDArray\n",
        "from rich.table import Table\n",
        "from rich.highlighter import ReprHighlighter\n",
        "from rich import box\n",
        "from tabulate import tabulate\n",
        "\n",
        "\n",
        "def dict2table(input_dict: dict, num_cols: int = 4, title: Optional[str] = None) -> Table:\n",
        "    num_items = len(input_dict)\n",
        "    num_rows = math.ceil(num_items / num_cols)\n",
        "    col = 0\n",
        "    data = {}\n",
        "    keys = []\n",
        "    vals = []\n",
        "\n",
        "    for i, (key, val) in enumerate(input_dict.items()):\n",
        "        keys.append(f'{key}:')\n",
        "\n",
        "        vals.append(val)\n",
        "        if (i + 1) % num_rows == 0:\n",
        "            data[col] = keys\n",
        "            data[col+1] = vals\n",
        "            keys = []\n",
        "            vals = []\n",
        "            col += 2\n",
        "\n",
        "    data[col] = keys\n",
        "    data[col+1] = vals\n",
        "\n",
        "    highlighter = ReprHighlighter()\n",
        "    message = tabulate(data, tablefmt='plain')\n",
        "    table = Table(title=title, show_header=False, box=box.HORIZONTALS)\n",
        "    table.add_row(highlighter(message))\n",
        "    return table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26sAkzJEM02_"
      },
      "outputs": [],
      "source": [
        "from rich.console import Console as RichConsole\n",
        "from rich.logging import RichHandler\n",
        "from rich.spinner import Spinner\n",
        "from rich.table import Table\n",
        "from rich.status import Status\n",
        "from rich.live import Live\n",
        "from rich._log_render import LogRender\n",
        "from time import time\n",
        "import logging\n",
        "\n",
        "\n",
        "class LogStatus(Status):\n",
        "    def __init__(self,\n",
        "        status,\n",
        "        console: RichConsole,\n",
        "        level: int = logging.INFO,\n",
        "        enabled: bool = True,\n",
        "        speed: float = 1.0,\n",
        "        refresh_per_second: float = 12.5,\n",
        "    ):\n",
        "        super().__init__(status,\n",
        "            console=console,\n",
        "            spinner='simpleDots',\n",
        "            speed=speed,\n",
        "            refresh_per_second=refresh_per_second\n",
        "        )\n",
        "\n",
        "        self.status = status\n",
        "        self.level = level\n",
        "        self.enabled = enabled\n",
        "        spinner = Spinner('simpleDots', style='status.spinner', speed=speed)\n",
        "        record = logging.LogRecord(name=None, level=level, pathname=None, lineno=None, msg=None, args=None, exc_info=None)\n",
        "        handler = RichHandler(console=console)\n",
        "        table = Table.grid()\n",
        "        table.add_row(self.status, spinner)\n",
        "\n",
        "        self._spinner = LogRender(show_level=True, time_format='[%X]')(\n",
        "            console=console,\n",
        "            level=handler.get_level_text(record),\n",
        "            renderables=[table]\n",
        "        )\n",
        "        self._live = Live(\n",
        "            self.renderable,\n",
        "            console=console,\n",
        "            refresh_per_second=refresh_per_second,\n",
        "            transient=True,\n",
        "        )\n",
        "\n",
        "    def __enter__(self):\n",
        "        if self.enabled:\n",
        "            self._start_time = time()\n",
        "            return super().__enter__()\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        if self.enabled:\n",
        "            super().__exit__(exc_type, exc_val, exc_tb)\n",
        "            self._end_time = time()\n",
        "            self.console.log(f'{self.status}...done in {self._end_time - self._start_time:.2f} s', level=self.level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IADowr3M7xf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from rich.console import Console as RichConsole\n",
        "from rich.logging import RichHandler\n",
        "from typing import Optional, Union\n",
        "from rich.console import JustifyMethod, NewLine\n",
        "from rich.scope import render_scope\n",
        "from rich.segment import Segment\n",
        "from rich.styled import Styled\n",
        "from rich.style import Style\n",
        "\n",
        "class Console(RichConsole):\n",
        "    CRITICAL = logging.CRITICAL\n",
        "    FATAL = logging.FATAL\n",
        "    ERROR = logging.ERROR\n",
        "    WARNING = logging.WARNING\n",
        "    WARN = logging.WARN\n",
        "    INFO = logging.INFO\n",
        "    DEBUG = logging.DEBUG\n",
        "    NOTSET = logging.NOTSET\n",
        "\n",
        "    def __init__(self, *args, log_level: int = INFO, **kwrags):\n",
        "        super().__init__(*args, **kwrags)\n",
        "        self.log_level = log_level\n",
        "\n",
        "    def status (self, status: str, level: int=INFO) -> LogStatus:\n",
        "        return LogStatus(status, console=self, level=level, enabled=level >= self.log_level)\n",
        "\n",
        "    def log(\n",
        "        self,\n",
        "        *objects,\n",
        "        level: int = logging.INFO,\n",
        "        sep: str = \" \",\n",
        "        end: str = \"\\n\",\n",
        "        style: Union[str, Style] = None,\n",
        "        justify: Optional[JustifyMethod] = None,\n",
        "        emoji: Optional[bool] = None,\n",
        "        markup: Optional[bool] = None,\n",
        "        highlight: Optional[bool] = None,\n",
        "        log_locals: bool = False,\n",
        "        _stack_offset: int = 2,\n",
        "    ) -> None:\n",
        "        \"\"\"Log rich content to the terminal.\n",
        "        Args:\n",
        "            objects (positional args): Objects to log to the terminal.\n",
        "            sep (str, optional): String to write between print data. Defaults to \" \".\n",
        "            end (str, optional): String to write at end of print data. Defaults to \"\\\\\\\\n\".\n",
        "            style (Union[str, Style], optional): A style to apply to output. Defaults to None.\n",
        "            justify (str, optional): One of \"left\", \"right\", \"center\", or \"full\". Defaults to ``None``.\n",
        "            emoji (Optional[bool], optional): Enable emoji code, or ``None`` to use console default. Defaults to None.\n",
        "            markup (Optional[bool], optional): Enable markup, or ``None`` to use console default. Defaults to None.\n",
        "            highlight (Optional[bool], optional): Enable automatic highlighting, or ``None`` to use console default. Defaults to None.\n",
        "            log_locals (bool, optional): Boolean to enable logging of locals where ``log()``\n",
        "                was called. Defaults to False.\n",
        "            _stack_offset (int, optional): Offset of caller from end of call stack. Defaults to 1.\n",
        "        \"\"\"\n",
        "        if level < self.log_level:\n",
        "            return\n",
        "\n",
        "        if not objects:\n",
        "            objects = (NewLine(),)\n",
        "\n",
        "        render_hooks = self._render_hooks[:]\n",
        "\n",
        "        with self:\n",
        "            renderables = self._collect_renderables(\n",
        "                objects, sep, end, justify=justify, emoji=emoji, markup=markup, highlight=highlight\n",
        "            )\n",
        "            if style is not None:\n",
        "                renderables = [Styled(renderable, style) for renderable in renderables]\n",
        "\n",
        "            filename, line_no, locals = self._caller_frame_info(_stack_offset)\n",
        "            link_path = None if filename.startswith(\"<\") else os.path.abspath(filename)\n",
        "            path = filename.rpartition(os.sep)[-1]\n",
        "            if log_locals:\n",
        "                locals_map = {key: value for key, value in locals.items() if not key.startswith(\"__\")}\n",
        "                renderables.append(render_scope(locals_map, title=\"[i]locals\"))\n",
        "\n",
        "            record = logging.LogRecord(name=None, level=level, pathname=None, lineno=None, msg=None, args=None, exc_info=None)\n",
        "            handler = RichHandler(console=self)\n",
        "            self._log_render.show_level = True\n",
        "\n",
        "            renderables = [\n",
        "                self._log_render(\n",
        "                    self,\n",
        "                    renderables,\n",
        "                    level=handler.get_level_text(record),\n",
        "                    log_time=self.get_datetime(),\n",
        "                    path=path,\n",
        "                    line_no=line_no,\n",
        "                    link_path=link_path,\n",
        "                )\n",
        "            ]\n",
        "            for hook in render_hooks:\n",
        "                renderables = hook.process_renderables(renderables)\n",
        "            new_segments = []\n",
        "            extend = new_segments.extend\n",
        "            render = self.render\n",
        "            render_options = self.options\n",
        "            for renderable in renderables:\n",
        "                extend(render(renderable, render_options))\n",
        "            buffer_extend = self._buffer.extend\n",
        "            for line in Segment.split_and_crop_lines(new_segments, self.width, pad=False):\n",
        "                buffer_extend(line)\n",
        "\n",
        "    def debug(self, *args, **kwargs):\n",
        "        self.log(*args, level=self.DEBUG, **kwargs)\n",
        "\n",
        "    def info(self, *args, **kwargs):\n",
        "        self.log(*args, level=self.INFO, **kwargs)\n",
        "\n",
        "    def warning(self, *args, **kwargs):\n",
        "        self.log(*args, level=self.WARNING, **kwargs)\n",
        "\n",
        "    def error(self, *args, **kwargs):\n",
        "        self.log(*args, level=self.ERROR, **kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcHOoxC4NYIa"
      },
      "source": [
        "Writing functions to perform data-preprocessing and filtering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xDuHUoSNcyZ"
      },
      "source": [
        "The following class acts upon the dataset, selecting the entries such that we specify the minimum clount of the class to be retained and also allows us to remove unlabeled data from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7IcdVMl5NXyw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class FilterClassByCount(BaseTransform):\n",
        "    def __init__(self, min_count: int, remove_unlabeled=False):\n",
        "        self.min_count = min_count\n",
        "        self.remove_unlabeled = remove_unlabeled\n",
        "\n",
        "    def __call__(self, data: Data) -> Data:\n",
        "        assert hasattr(data, 'y')\n",
        "\n",
        "        y: torch.Tensor = F.one_hot(data.y)\n",
        "        counts = y.sum(dim=0)\n",
        "        y = y[:, counts >= self.min_count]\n",
        "        mask = y.sum(dim=1).bool()        # nodes to keep\n",
        "        data.y = y.argmax(dim=1)\n",
        "\n",
        "        if self.remove_unlabeled:\n",
        "            data = data.subgraph(mask)\n",
        "        else:\n",
        "            data.y[~mask] = -1                # set filtered nodes as unlabeled\n",
        "            if hasattr(data, 'train_mask'):\n",
        "                data.train_mask = data.train_mask & mask\n",
        "                data.val_mask = data.val_mask & mask\n",
        "                data.test_mask = data.test_mask & mask\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRK-ml-fOI94"
      },
      "source": [
        "The following class creates a mask for the entire graph, sets its value as TRUE for the nodes connected via edges, and applies this mask onto the graph to filter out the nodes that are not connected to any other node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhM8uAMEOGTi"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.transforms import BaseTransform\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class RemoveIsolatedNodes(BaseTransform):\n",
        "    def __call__(self, data: Data) -> Data:\n",
        "        mask = data.y.new_zeros(data.num_nodes, dtype=bool)\n",
        "        mask[data.edge_index[0]] = True\n",
        "        mask[data.edge_index[1]] = True\n",
        "        data = data.subgraph(mask)\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yBuiG61Oi-D"
      },
      "source": [
        "The following class removes self loops from the graph data structure, if any."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "du_HnJNsOifH"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import remove_self_loops\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class RemoveSelfLoops(BaseTransform):\n",
        "    def __call__(self, data: Data) -> Data:\n",
        "        if hasattr(data, 'edge_index') and data.edge_index is not None:\n",
        "            data.edge_index, _ = remove_self_loops(data.edge_index)\n",
        "        if hasattr(data, 'adj_t'):\n",
        "            data.adj_t = data.adj_t.remove_diag()\n",
        "        return data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fGUyp0iO03j"
      },
      "source": [
        "Now, for transductive setting, we require that the same graph entity be used for training, testing and validation. Hence, we need to predict nodes' labels, for those nodes that are already existent in the graph structure provided.\n",
        "\n",
        "* Hence, we need to reserve subsets of nodes on the given graph, for training, testing and validation - a purpose for which we create masks.\n",
        "\n",
        "* The following class essentially facilitates the random split of graph data into training, testing and validation sets based on the specified ratios and ensures that each class has a balanced representation in each split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cA8gBVjbOsWX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.transforms import BaseTransform\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class RandomDataSplit(BaseTransform):\n",
        "    def __init__(self, num_nodes_per_class, train_ratio=0.7, test_ratio=0.2):\n",
        "        self.num_nodes_per_class = num_nodes_per_class\n",
        "        self.train_ratio = train_ratio\n",
        "        self.test_ratio = test_ratio\n",
        "\n",
        "    def __call__(self, data: Data) -> Data:\n",
        "        y = data.y\n",
        "        num_classes = y.max().item() + 1\n",
        "        num_train_nodes_per_class = int(self.num_nodes_per_class * self.train_ratio)\n",
        "        num_test_nodes_per_class = int(self.num_nodes_per_class * self.test_ratio)\n",
        "\n",
        "        train_mask = torch.zeros_like(y, dtype=torch.bool)\n",
        "        test_mask = torch.zeros_like(y, dtype=torch.bool)\n",
        "        val_mask = torch.zeros_like(y, dtype=torch.bool)\n",
        "\n",
        "        for c in range(num_classes):\n",
        "            idx = (y == c).nonzero(as_tuple=False).view(-1)\n",
        "            num_nodes = idx.size(0)\n",
        "            if num_nodes >= self.num_nodes_per_class:\n",
        "                idx = idx[torch.randperm(idx.size(0))][:self.num_nodes_per_class]\n",
        "                train_mask[idx[:num_train_nodes_per_class]] = True\n",
        "                test_mask[idx[num_train_nodes_per_class:num_train_nodes_per_class + num_test_nodes_per_class]] = True\n",
        "                val_mask[idx[num_train_nodes_per_class + num_test_nodes_per_class:]] = True\n",
        "\n",
        "        data.train_mask = train_mask\n",
        "        data.test_mask = test_mask\n",
        "        data.val_mask = val_mask\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sNDM1BrGQfWh"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated, Callable, Literal, Union, get_args, get_origin\n",
        "import inspect\n",
        "from argparse import SUPPRESS, ArgumentParser, ArgumentTypeError, Namespace\n",
        "\n",
        "\n",
        "ArgType = Union[Namespace, dict[str, object]]\n",
        "ArgInfo = dict[str, object]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyEgCuafMNEh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from functools import partial\n",
        "from typing import Annotated\n",
        "import torch\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.datasets import Reddit\n",
        "from torch_geometric.transforms import Compose, ToSparseTensor, RandomNodeSplit\n",
        "\n",
        "def load_wenet(root: str, transform=None) -> Data:\n",
        "    data = torch.load(os.path.join(root, 'data.pt'))\n",
        "    data = data if transform is None else transform(data)\n",
        "    return [data]\n",
        "\n",
        "\n",
        "class DatasetLoader:\n",
        "    supported_datasets = {\n",
        "        'reddit': partial(Reddit,\n",
        "            pre_transform=Compose([\n",
        "                RandomNodeSplit(num_val=0.1, num_test=0.15),\n",
        "                FilterClassByCount(min_count=10000, remove_unlabeled=True),\n",
        "                RemoveSelfLoops(), RemoveIsolatedNodes(),\n",
        "            ])\n",
        "        ),\n",
        "        'facebook': partial(Facebook, name='UIllinois20', target='year',\n",
        "            pre_transform=Compose([\n",
        "                RandomNodeSplit(num_val=0.1, num_test=0.15),\n",
        "                FilterClassByCount(min_count=1000, remove_unlabeled=True),\n",
        "                RemoveSelfLoops(), RemoveIsolatedNodes(),\n",
        "            ])\n",
        "        ),\n",
        "        'fb-100': partial(FB100, target='year',\n",
        "            pre_transform=Compose([\n",
        "                RandomNodeSplit(num_val=0.1, num_test=0.15),\n",
        "                FilterClassByCount(min_count=100000, remove_unlabeled=True),\n",
        "                RemoveSelfLoops(), RemoveIsolatedNodes(),\n",
        "            ])\n",
        "        )\n",
        "    }\n",
        "    console = Console()\n",
        "\n",
        "    def __init__(self,\n",
        "                 dataset:    Annotated[str, ArgInfo(help='name of the dataset', choices=supported_datasets)] = 'facebook',\n",
        "                 data_dir:   Annotated[str, ArgInfo(help='directory to store the dataset')] = './datasets',\n",
        "                 ):\n",
        "\n",
        "        self.name = dataset\n",
        "        self.data_dir = data_dir\n",
        "        self.console = Console()\n",
        "\n",
        "    def load(self, verbose=False) -> Data:\n",
        "        data = self.supported_datasets[self.name](root=os.path.join(self.data_dir, self.name))[0]\n",
        "        data = ToSparseTensor(layout=torch.sparse_csr)(data)\n",
        "\n",
        "        if verbose:\n",
        "            self.print_stats(data)\n",
        "\n",
        "        return data\n",
        "\n",
        "    def print_stats(self, data: Data):\n",
        "        nodes_degree: torch.Tensor = data.adj_t.to_sparse_coo().sum(dim=1).to_dense() # in degree\n",
        "        baseline: float = (data.y[data.test_mask].unique(return_counts=True)[1].max().item() * 100 / data.test_mask.sum().item())\n",
        "        train_ratio: float = data.train_mask.sum().item() / data.num_nodes * 100\n",
        "        val_ratio: float = data.val_mask.sum().item() / data.num_nodes * 100\n",
        "        test_ratio: float = data.test_mask.sum().item() / data.num_nodes * 100\n",
        "\n",
        "        stat = {\n",
        "            'nodes': f'{data.num_nodes:,}',\n",
        "            'edges': f'{data.num_edges:,}',\n",
        "            'features': f'{data.num_features:,}',\n",
        "            'classes': f'{int(data.y.max() + 1)}',\n",
        "            'mean degree': f'{nodes_degree.mean():.2f}',\n",
        "            'median degree': f'{nodes_degree.median()}',\n",
        "            'train/val/test (%)': f'{train_ratio:.1f}/{val_ratio:.1f}/{test_ratio:.1f}',\n",
        "            'baseline acc (%)': f'{baseline:.2f}'\n",
        "        }\n",
        "\n",
        "        table = dict2table(stat, num_cols=2, title=f'dataset: [yellow]{self.name}[/yellow]')\n",
        "        self.console.info(table)\n",
        "        self.console.print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PhARsoqzST_f",
        "outputId": "aeb097dd-90bd-4a67-987d-865da7a36095"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Villanova62.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UCLA26.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Tennessee95.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//NYU9.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Carnegie49.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//GWU54.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//USF51.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Vanderbilt48.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//USC35.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Rutgers89.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UConn91.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//MIT8.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//USFCA72.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UChicago30.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UIllinois20.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UC61.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Cal65.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Yale4.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Northeastern19.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Dartmouth6.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Vermont70.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Northwestern25.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//William77.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Harvard1.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Princeton12.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UC64.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Middlebury45.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Haverford76.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Bingham82.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UNC28.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Berkeley13.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Rochester38.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Swarthmore42.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Virginia63.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//WashU32.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Columbia2.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//NotreDame57.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Bucknell39.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UVA16.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Maine59.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//MU78.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Simmons81.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//MSU24.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Colgate88.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Temple83.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Cornell5.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Indiana69.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Oklahoma97.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Michigan23.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//BU10.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Brown11.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Auburn71.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//FSU53.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UGA50.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UCF52.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Howard90.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UCSD34.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Vassar85.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Tufts18.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UPenn7.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Baylor93.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UMass92.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Bowdoin47.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Maryland58.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Penn94.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Wesleyan43.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UC33.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Rice31.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UCSC68.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Smith60.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Caltech36.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Hamilton46.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Oberlin44.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//American75.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Mich67.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Mississippi66.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Williams40.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UCSB37.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Amherst41.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Duke14.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Pepperdine86.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Wake73.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Lehigh96.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Reed98.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Tulane29.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Texas84.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Wellesley22.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//JMU79.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Santa74.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Wisconsin87.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Stanford3.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Texas80.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//UF21.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//JohnsHopkins55.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Syracuse56.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//BC17.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Georgetown15.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Trinity100.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Brandeis99.mat\n",
            "Downloading https://github.com/sisaman/pyg-datasets/raw/main/datasets/facebook100//Emory27.mat\n",
            "Processing...\n",
            "Done!\n",
            "/usr/local/lib/python3.10/dist-packages/torch_geometric/utils/sparse.py:268: UserWarning: Sparse CSR tensor support is in beta state. If you miss a functionality in the sparse tensor support, please submit a feature request to https://github.com/pytorch/pytorch/issues. (Triggered internally at ../aten/src/ATen/SparseCsrTensorImpl.cpp:53.)\n",
            "  adj = torch.sparse_csr_tensor(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:00:32] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-style: italic\">                      dataset: </span><span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">fb-100</span><span style=\"font-style: italic\">                       </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-13-a5a07c187594&gt;:78</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           nodes:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280</span>   mean degree:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77.04</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           edges:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">478</span>  median degree:       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57.0</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           features:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">537</span>         train/val/test <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.0</span>                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           classes:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>           baseline acc <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.45</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:00:32]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[3m                      dataset: \u001b[0m\u001b[3;33mfb-100\u001b[0m\u001b[3m                       \u001b[0m \u001b[2m<ipython-input-13-a5a07c187594>:78\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           nodes:     \u001b[1;36m1\u001b[0m,\u001b[1;36m120\u001b[0m,\u001b[1;36m280\u001b[0m   mean degree:         \u001b[1;36m77.04\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           edges:     \u001b[1;36m86\u001b[0m,\u001b[1;36m304\u001b[0m,\u001b[1;36m478\u001b[0m  median degree:       \u001b[1;36m57.0\u001b[0m           \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           features:  \u001b[1;36m537\u001b[0m         train/val/test \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:                 \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           \u001b[1;36m75.0\u001b[0m/\u001b[1;36m10.0\u001b[0m/\u001b[1;36m15.0\u001b[0m                                             \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           classes:   \u001b[1;36m6\u001b[0m           baseline acc \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:    \u001b[1;36m19.45\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize the DatasetLoader class with the desired dataset\n",
        "loader = DatasetLoader(dataset='fb-100')\n",
        "\n",
        "# Load the dataset\n",
        "data = loader.load(verbose=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POJwOy5vDYFv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "49G18jaofnkS",
        "outputId": "a73f6405-8208-4bbb-a3c5-9f8c9cbaa3c3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.dgl.ai/dataset/reddit.zip\n",
            "Extracting datasets/reddit/raw/reddit.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:01:26] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-style: italic\">                      dataset: </span><span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">reddit</span><span style=\"font-style: italic\">                       </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-13-a5a07c187594&gt;:78</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           nodes:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">116</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">713</span>     mean degree:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">396.13</span>         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           edges:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">233</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">380</span>  median degree:       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209.0</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           features:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">602</span>         train/val/test <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.0</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           classes:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>           baseline acc <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.37</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:01:26]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[3m                      dataset: \u001b[0m\u001b[3;33mreddit\u001b[0m\u001b[3m                       \u001b[0m \u001b[2m<ipython-input-13-a5a07c187594>:78\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           nodes:     \u001b[1;36m116\u001b[0m,\u001b[1;36m713\u001b[0m     mean degree:         \u001b[1;36m396.13\u001b[0m         \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           edges:     \u001b[1;36m46\u001b[0m,\u001b[1;36m233\u001b[0m,\u001b[1;36m380\u001b[0m  median degree:       \u001b[1;36m209.0\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           features:  \u001b[1;36m602\u001b[0m         train/val/test \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:                 \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           \u001b[1;36m75.1\u001b[0m/\u001b[1;36m9.9\u001b[0m/\u001b[1;36m15.0\u001b[0m                                              \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           classes:   \u001b[1;36m8\u001b[0m           baseline acc \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:    \u001b[1;36m24.37\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:01:29] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-style: italic\">                      dataset: </span><span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">reddit</span><span style=\"font-style: italic\">                       </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-13-a5a07c187594&gt;:78</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           nodes:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">116</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">713</span>     mean degree:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">396.13</span>         <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           edges:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">46</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">233</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">380</span>  median degree:       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">209.0</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           features:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">602</span>         train/val/test <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.1</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9.9</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.0</span>                                              <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           classes:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>           baseline acc <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">24.37</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:01:29]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[3m                      dataset: \u001b[0m\u001b[3;33mreddit\u001b[0m\u001b[3m                       \u001b[0m \u001b[2m<ipython-input-13-a5a07c187594>:78\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           nodes:     \u001b[1;36m116\u001b[0m,\u001b[1;36m713\u001b[0m     mean degree:         \u001b[1;36m396.13\u001b[0m         \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           edges:     \u001b[1;36m46\u001b[0m,\u001b[1;36m233\u001b[0m,\u001b[1;36m380\u001b[0m  median degree:       \u001b[1;36m209.0\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           features:  \u001b[1;36m602\u001b[0m         train/val/test \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:                 \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           \u001b[1;36m75.1\u001b[0m/\u001b[1;36m9.9\u001b[0m/\u001b[1;36m15.0\u001b[0m                                              \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           classes:   \u001b[1;36m8\u001b[0m           baseline acc \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:    \u001b[1;36m24.37\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "reddit_loader = DatasetLoader(dataset = 'reddit')\n",
        "reddit_data = reddit_loader.load(verbose = True)\n",
        "reddit_loader.print_stats(reddit_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBWKA8j5S6wW"
      },
      "source": [
        "* Now, the dataset has been loaded, complete with the connectivity (adjacency\n",
        "  matrix) - `adj_t`, `train_mask`, `test_mask`, `val_mask` and `y`.\n",
        "* The feature matrix is stored under `data.x`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f3kJBvWS0XI",
        "outputId": "74b8409a-93e1-46f0-eb47-d5330262abab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[1120280, 537], y=[1120280], train_mask=[1120280], val_mask=[1120280], test_mask=[1120280], adj_t=[1120280, 1120280])\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "id": "nFqil_4gTJ64",
        "outputId": "e1ffe797-6f4d-44c9-a673-65f2f1707dd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[15:01:35] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> <span style=\"font-style: italic\">                      dataset: </span><span style=\"color: #808000; text-decoration-color: #808000; font-style: italic\">fb-100</span><span style=\"font-style: italic\">                       </span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">&lt;ipython-input-13-a5a07c187594&gt;:78</span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           nodes:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">280</span>   mean degree:         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">77.04</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           edges:     <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">86</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">304</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">478</span>  median degree:       <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57.0</span>           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           features:  <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">537</span>         train/val/test <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">75.0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10.0</span>/<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">15.0</span>                                             <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>           classes:   <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span>           baseline acc <span style=\"font-weight: bold\">(</span>%<span style=\"font-weight: bold\">)</span>:    <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">19.45</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">           </span>          ──────────────────────────────────────────────────────────  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">                                  </span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[2;36m[15:01:35]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m \u001b[3m                      dataset: \u001b[0m\u001b[3;33mfb-100\u001b[0m\u001b[3m                       \u001b[0m \u001b[2m<ipython-input-13-a5a07c187594>:78\u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           nodes:     \u001b[1;36m1\u001b[0m,\u001b[1;36m120\u001b[0m,\u001b[1;36m280\u001b[0m   mean degree:         \u001b[1;36m77.04\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           edges:     \u001b[1;36m86\u001b[0m,\u001b[1;36m304\u001b[0m,\u001b[1;36m478\u001b[0m  median degree:       \u001b[1;36m57.0\u001b[0m           \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           features:  \u001b[1;36m537\u001b[0m         train/val/test \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:                 \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           \u001b[1;36m75.0\u001b[0m/\u001b[1;36m10.0\u001b[0m/\u001b[1;36m15.0\u001b[0m                                             \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m           classes:   \u001b[1;36m6\u001b[0m           baseline acc \u001b[1m(\u001b[0m%\u001b[1m)\u001b[0m:    \u001b[1;36m19.45\u001b[0m          \u001b[2m                                  \u001b[0m\n",
              "\u001b[2;36m           \u001b[0m          ──────────────────────────────────────────────────────────  \u001b[2m                                  \u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "loader.print_stats(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1d5mzDu4lK5h",
        "outputId": "f35f86e9-e3a0-40d6-d42a-5c53f58dcf84"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "86304478"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.num_edges"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjLaCNq-Utm2"
      },
      "source": [
        "Now, as present in the ProGAP paper, which establishes a baseline using MLP Classifier (not taking adjacency information into account) for edge-private case, we work to write code to perform such node-classfication using MLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKi5WjyvWcV-"
      },
      "source": [
        "* First, we write the class `TrainerProgress` to define a custom progress bar that aids our tracking of the training/testing processes while training any model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEpuhdGdWC9d"
      },
      "outputs": [],
      "source": [
        "from typing import Literal, TypeVar\n",
        "from torch.types import Number\n",
        "\n",
        "\n",
        "RT = TypeVar('RT')\n",
        "Phase = Literal['train', 'val', 'test', 'predict']\n",
        "Metrics = dict[str, Number]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12y5_Eg7V63P"
      },
      "outputs": [],
      "source": [
        "from typing import Iterable\n",
        "from rich.console import Group\n",
        "from rich.padding import Padding\n",
        "from rich.table import Column, Table\n",
        "from rich.progress import Progress, SpinnerColumn, BarColumn, TimeElapsedColumn, Task\n",
        "from rich.highlighter import ReprHighlighter\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TrainerProgress(Progress):\n",
        "    def __init__(self,\n",
        "                 num_epochs: int,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "\n",
        "        progress_bar = [\n",
        "            SpinnerColumn(),\n",
        "            \"{task.description}\",\n",
        "            \"[cyan]{task.completed:>3}[/cyan]/[cyan]{task.total}[/cyan]\",\n",
        "            \"{task.fields[unit]}\",\n",
        "            BarColumn(),\n",
        "            \"[cyan]{task.percentage:>3.0f}[/cyan]%\",\n",
        "            TimeElapsedColumn(),\n",
        "            # \"{task.fields[metrics]}\"\n",
        "        ]\n",
        "\n",
        "        console = Console()\n",
        "\n",
        "        super().__init__(*progress_bar, console=console, **kwargs)\n",
        "\n",
        "        self.trainer_tasks = {\n",
        "            'epoch': self.add_task(total=num_epochs, metrics='', unit='epochs', description='overal progress'),\n",
        "            'train': self.add_task(metrics='', unit='steps', description='training', visible=False),\n",
        "            'val':   self.add_task(metrics='', unit='steps', description='validation', visible=False),\n",
        "            'test':  self.add_task(metrics='', unit='steps', description='testing', visible=False),\n",
        "        }\n",
        "\n",
        "        self.max_rows = 0\n",
        "\n",
        "    def update(self, task: Task, **kwargs):\n",
        "        if 'metrics' in kwargs:\n",
        "            kwargs['metrics'] = self.render_metrics(kwargs['metrics'])\n",
        "\n",
        "        super().update(self.trainer_tasks[task], **kwargs)\n",
        "\n",
        "    def reset(self, task: Task, **kwargs):\n",
        "        super().reset(self.trainer_tasks[task], **kwargs)\n",
        "\n",
        "    def render_metrics(self, metrics: Metrics) -> str:\n",
        "        out = []\n",
        "        for split in ['train', 'val', 'test']:\n",
        "            metric_str = ' '.join(f'{k}: {v:.3f}' for k, v in metrics.items() if f'{split}/' in k)\n",
        "            out.append(metric_str)\n",
        "\n",
        "        return '  '.join(out)\n",
        "\n",
        "    def make_tasks_table(self, tasks: Iterable[Task]) -> Table:\n",
        "        \"\"\"Get a table to render the Progress display.\n",
        "\n",
        "        Args:\n",
        "            tasks (Iterable[Task]): An iterable of Task instances, one per row of the table.\n",
        "\n",
        "        Returns:\n",
        "            Table: A table instance.\n",
        "        \"\"\"\n",
        "        table_columns = (\n",
        "            (\n",
        "                Column(no_wrap=True)\n",
        "                if isinstance(_column, str)\n",
        "                else _column.get_table_column().copy()\n",
        "            )\n",
        "            for _column in self.columns\n",
        "        )\n",
        "\n",
        "        highlighter = ReprHighlighter()\n",
        "        table = Table.grid(*table_columns, padding=(0, 1), expand=self.expand)\n",
        "\n",
        "        if tasks:\n",
        "            epoch_task = tasks[0]\n",
        "            metrics = epoch_task.fields['metrics']\n",
        "\n",
        "            for task in tasks:\n",
        "                if task.visible:\n",
        "                    table.add_row(\n",
        "                        *(\n",
        "                            (\n",
        "                                column.format(task=task)\n",
        "                                if isinstance(column, str)\n",
        "                                else column(task)\n",
        "                            )\n",
        "                            for column in self.columns\n",
        "                        )\n",
        "                    )\n",
        "\n",
        "            self.max_rows = max(self.max_rows, table.row_count)\n",
        "            pad_top = 0 if epoch_task.finished else self.max_rows - table.row_count\n",
        "            group = Group(table, Padding(highlighter(metrics), pad=(pad_top,0,0,2)))\n",
        "            return Padding(group, pad=(0,0,1,18))\n",
        "\n",
        "        else:\n",
        "            return table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K9FIvUBYVZK"
      },
      "source": [
        "Since we are training multiple models on the given dataset, we require a template class that can be used to create trainable modules in PyTorch.\n",
        "The following class `TrainableModule` does just that.\n",
        "\n",
        "* This class abstracts away common functionalities related to optimization, such as configuring optimizers, handling forward passes, taking optimization steps, and making predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9_8SBEgXS78"
      },
      "outputs": [],
      "source": [
        "from typing import Optional\n",
        "from torch import Tensor\n",
        "from torch.nn import Module\n",
        "from torch.optim.optimizer import Optimizer\n",
        "from torch_geometric.data import Data\n",
        "from abc import ABC, abstractmethod\n",
        "from class_resolver.contrib.torch import optimizer_resolver\n",
        "\n",
        "# Defined as a subclass of both Module and ABC, inheriting attributes from both.\n",
        "\n",
        "# Serves as a base class for all the trainable modules in the project (nonPrivate, Edge-Private etc.)\n",
        "class TrainableModule(Module, ABC):\n",
        "    def __init__(self, optimizer: str, learning_rate: float, weight_decay: float):\n",
        "        super().__init__()\n",
        "        self.optimizer_name = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "    @abstractmethod\n",
        "    def forward(self, *args, **kwargs): pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, data: Data, phase: Phase) -> tuple[Optional[Tensor], Metrics]: pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, data: Data) -> Tensor: pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset_parameters(self): pass\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = optimizer_resolver.make(\n",
        "            query=self.optimizer_name,\n",
        "            params=self.parameters(),\n",
        "            lr=self.learning_rate,\n",
        "            weight_decay=self.weight_decay\n",
        "        )\n",
        "        return optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_tKT8DKmwCN"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from torch.nn import Module\n",
        "\n",
        "\n",
        "class LoggerBase(ABC):\n",
        "    @property\n",
        "    @abstractmethod\n",
        "    def experiment(self): pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def log(self, metrics: dict[str, object]): pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def log_summary(self, metrics: dict[str, object]): pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def watch(self, model: Module, **kwargs): pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def finish(self): pass\n",
        "\n",
        "class DummyLogger(LoggerBase):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    @property\n",
        "    def experiment(self): pass\n",
        "    def log(self, metrics: dict[str, object]): pass\n",
        "    def log_summary(self, metrics: dict[str, object]): pass\n",
        "    def watch(self, model: Module, **kwargs): pass\n",
        "    def finish(self): pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-pR8u-UnC_l"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import wandb\n",
        "    from wandb.wandb_run import Run\n",
        "except ImportError:\n",
        "    wandb = None\n",
        "\n",
        "\n",
        "class WandbLogger(LoggerBase):\n",
        "    def __init__(self, project: str, output_dir: str, config: dict = {}):\n",
        "        self.project = project\n",
        "        self.output_dir = output_dir\n",
        "        self.config = config\n",
        "\n",
        "        if wandb is None:\n",
        "            raise ImportError(\n",
        "                \"wandb is not installed yet, install it with `pip install wandb`.\"\n",
        "            )\n",
        "\n",
        "    @property\n",
        "    def experiment(self) -> Run:\n",
        "        if not hasattr(self, \"_experiment\"):\n",
        "            os.environ[\"WANDB_SILENT\"] = \"true\"\n",
        "            settings = wandb.Settings(start_method=\"fork\")\n",
        "            os.makedirs(self.output_dir, exist_ok=True)\n",
        "\n",
        "            self._experiment = wandb.init(\n",
        "                project=self.project,\n",
        "                dir=self.output_dir,\n",
        "                reinit=True,\n",
        "                resume=\"allow\",\n",
        "                config=self.config,\n",
        "                save_code=True,\n",
        "                settings=settings,\n",
        "            )\n",
        "\n",
        "        return self._experiment\n",
        "\n",
        "    def log(self, metrics: dict[str, object]):\n",
        "        self.experiment.log(metrics)\n",
        "\n",
        "    def log_summary(self, metrics: dict[str, object]):\n",
        "        for metric, value in metrics.items():\n",
        "            self.experiment.summary[metric] = value\n",
        "\n",
        "    def watch(self, model: Module, **kwargs):\n",
        "        self.experiment.watch(model, **kwargs)\n",
        "\n",
        "    def finish(self):\n",
        "        self.experiment.finish()\n",
        "\n",
        "class Logger(LoggerBase):\n",
        "    supported_loggers = {\n",
        "        'wandb': WandbLogger\n",
        "    }\n",
        "\n",
        "    def __init__(self,\n",
        "        logger:     Annotated[str,  ArgInfo(help='select logger type', choices=supported_loggers)] = None,\n",
        "        project:    Annotated[str,  ArgInfo(help=\"project name for logger\")] = 'ProGAP',\n",
        "        output_dir: Annotated[str,  ArgInfo(help=\"directory to store the results\")] = './output',\n",
        "        config:     dict = {},\n",
        "        prefix:     str = '',\n",
        "        ) -> None:\n",
        "\n",
        "        self.logger_name = logger\n",
        "        self.project = project\n",
        "        self.output_dir = output_dir\n",
        "        self.config = config\n",
        "        self.prefix = prefix\n",
        "\n",
        "        if self.logger_name == 'wandb':\n",
        "            self.logger = WandbLogger(\n",
        "                project=project,\n",
        "                output_dir=output_dir,\n",
        "                config=config\n",
        "            )\n",
        "        else:\n",
        "            self.logger = DummyLogger()\n",
        "\n",
        "    @property\n",
        "    def experiment(self):\n",
        "        return self.logger.experiment\n",
        "\n",
        "    def log(self, metrics: dict[str, object]):\n",
        "        metrics = self._add_prefix(metrics)\n",
        "        self.logger.log(metrics)\n",
        "\n",
        "    def log_summary(self, metrics: dict[str, object]):\n",
        "        metrics = self._add_prefix(metrics)\n",
        "        self.logger.log_summary(metrics)\n",
        "\n",
        "    def watch(self, model: Module, **kwargs):\n",
        "        self.logger.watch(model, **kwargs)\n",
        "\n",
        "    def finish(self):\n",
        "        self.logger.finish()\n",
        "\n",
        "    def set_prefix(self, prefix: str):\n",
        "        self.prefix = prefix\n",
        "\n",
        "    def _add_prefix(self, metrics: dict[str, object]) -> dict[str, object]:\n",
        "        return {f'{self.prefix}{metric}': value for metric, value in metrics.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJbdnkg2ZO3b"
      },
      "source": [
        "Writing code to train our models -->\n",
        "This class `Trainer` has an attribute defined, by the name of `model`, which is an initialization of the `TrainableModule` class. Hence, `Trainer` accesses any such module that is requried to be trained via this attribute.\n",
        "\n",
        "* Secondly, all the information is cast to the `cuda` device, if present on the system.\n",
        "\n",
        "* The reason I chose not to utilize logger for now is because I am not using any HPC cluster for my job."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvGWxPX8VyLK"
      },
      "outputs": [],
      "source": [
        "from copy import deepcopy\n",
        "from typing import Iterable, Optional, Annotated, Literal\n",
        "from torch.optim import Optimizer\n",
        "from torch.types import Number\n",
        "from torchmetrics import MeanMetric\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    def __init__(self,\n",
        "                 monitor:       str = 'val/acc',\n",
        "                 monitor_mode:  Literal['min', 'max'] = 'max',\n",
        "                 epochs:        Annotated[int,  ArgInfo(help='number of epochs for training')] = 100,\n",
        "                 device:        Annotated[str,  ArgInfo(help='device to use for training', choices=['cpu', 'cuda', 'auto'])] = 'auto',\n",
        "                 verbose:       Annotated[bool, ArgInfo(help='display progress')] = True,\n",
        "                 logger:        Logger = None,\n",
        "                 ):\n",
        "\n",
        "        self.epochs = epochs\n",
        "        self.monitor = monitor\n",
        "        self.monitor_mode = monitor_mode\n",
        "        self.verbose = verbose\n",
        "        self.logger = logger or DummyLogger()\n",
        "\n",
        "        # setup device\n",
        "        if device == 'auto':\n",
        "            self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "        else:\n",
        "            self.device = torch.device(device)\n",
        "\n",
        "        # trainer internal state\n",
        "        self.model: TrainableModule = None\n",
        "        self.metrics: dict[str, MeanMetric] = {}\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.model = None\n",
        "        self.metrics.clear()\n",
        "\n",
        "    def update_metrics(self, metric_name: str, metric_value: object, batch_size: int = 1) -> None:\n",
        "        # if this is a new metric, add it to self.metrics\n",
        "        device = metric_value.device if torch.is_tensor(metric_value) else 'cpu'\n",
        "        if metric_name not in self.metrics:\n",
        "            self.metrics[metric_name] = MeanMetric().to(device)\n",
        "\n",
        "        # update the metric\n",
        "        self.metrics[metric_name].update(metric_value, weight=batch_size)\n",
        "\n",
        "    def aggregate_metrics(self, phase: Phase='train') -> Metrics:\n",
        "        metrics = {}\n",
        "\n",
        "        for metric_name, metric_value in self.metrics.items():\n",
        "            if phase in metric_name.split('/'):\n",
        "                value = metric_value.compute()\n",
        "                metric_value.reset()\n",
        "                metrics[metric_name] = value\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def is_better(self, current_metric: Number, previous_metric: Number) -> bool:\n",
        "        assert self.monitor_mode in ['min', 'max'], f'Unknown metric mode: {self.monitor_mode}'\n",
        "        if self.monitor_mode == 'max':\n",
        "            return current_metric > previous_metric\n",
        "        elif self.monitor_mode == 'min':\n",
        "            return current_metric < previous_metric\n",
        "\n",
        "    def fit(self,\n",
        "            model: TrainableModule,\n",
        "            train_dataloader: Iterable,\n",
        "            val_dataloader: Optional[Iterable]=None,\n",
        "            test_dataloader: Optional[Iterable]=None,\n",
        "            ) -> Metrics:\n",
        "\n",
        "        self.model = model.to(self.device)\n",
        "        self.optimizer: Optimizer = self.model.configure_optimizers()\n",
        "\n",
        "        self.progress = TrainerProgress(\n",
        "            num_epochs=self.epochs,\n",
        "            disable=not self.verbose,\n",
        "        )\n",
        "\n",
        "        best_state_dict = None\n",
        "        best_metrics = None\n",
        "\n",
        "        with self.progress:\n",
        "            for epoch in range(1, self.epochs + 1):\n",
        "                metrics = {f'epoch': epoch}\n",
        "\n",
        "                # train loop\n",
        "                train_metrics = self.loop(train_dataloader, phase='train')\n",
        "                metrics.update(train_metrics)\n",
        "\n",
        "                # validation loop\n",
        "                if val_dataloader:\n",
        "                    val_metrics = self.loop(val_dataloader, phase='val')\n",
        "                    metrics.update(val_metrics)\n",
        "\n",
        "                    if best_metrics is None or self.is_better(\n",
        "                        metrics[self.monitor], best_metrics[self.monitor]\n",
        "                        ):\n",
        "                        best_metrics = metrics\n",
        "                        best_state_dict = deepcopy(self.model.state_dict())\n",
        "\n",
        "                # test loop\n",
        "                if test_dataloader:\n",
        "                    test_metrics = self.loop(test_dataloader, phase='test')\n",
        "                    metrics.update(test_metrics)\n",
        "\n",
        "                # log and update progress\n",
        "                self.progress.update(task='epoch', metrics=metrics, advance=1)\n",
        "                self.logger.log(metrics)\n",
        "\n",
        "        if best_metrics is None:\n",
        "            best_metrics = metrics\n",
        "        else:\n",
        "            self.model.load_state_dict(best_state_dict)\n",
        "\n",
        "        # log and return best metrics\n",
        "        self.logger.log_summary(best_metrics)\n",
        "\n",
        "        return best_metrics\n",
        "\n",
        "    def test(self, dataloader: Iterable) -> Metrics:\n",
        "        self.metrics.clear()\n",
        "        metrics = self.loop(dataloader, phase='test')\n",
        "        return metrics\n",
        "\n",
        "    def predict(self, dataloader: Iterable, move_to_cpu: bool=False) -> Metrics:\n",
        "        preds = []\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                batch = self.to_device(batch)\n",
        "                # out might be a tuple of predictions\n",
        "                out = self.model.predict(batch)\n",
        "                if move_to_cpu:\n",
        "                    out = out.cpu()\n",
        "                preds.append(out)\n",
        "\n",
        "        # concatenate predictions, check if they are tuples\n",
        "        if isinstance(preds[0], tuple):\n",
        "            preds = tuple(torch.cat([p[i] for p in preds]) for i in range(len(preds[0])))\n",
        "        else:\n",
        "            preds = torch.cat(preds)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    def loop(self, dataloader: Iterable, phase: Phase) -> Metrics:\n",
        "        self.model.train(phase == 'train')\n",
        "        grad_state = torch.is_grad_enabled()\n",
        "        torch.set_grad_enabled(phase == 'train')\n",
        "        self.progress.update(phase, visible=len(dataloader) > 1, total=len(dataloader))\n",
        "\n",
        "        for batch in dataloader:\n",
        "            batch = self.to_device(batch)\n",
        "            metrics = self.step(batch, phase)\n",
        "            for item in metrics:\n",
        "                self.update_metrics(item, metrics[item], batch_size=batch.batch_nodes.size(0))\n",
        "            self.progress.update(phase, advance=1)\n",
        "\n",
        "        self.progress.reset(phase, visible=False)\n",
        "        torch.set_grad_enabled(grad_state)\n",
        "        return self.aggregate_metrics(phase)\n",
        "\n",
        "    def step(self, batch, phase: Phase) -> Metrics:\n",
        "        if phase == 'train':\n",
        "            self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        loss, metrics = self.model.step(batch, phase=phase)\n",
        "\n",
        "        if phase == 'train':\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def to_device(self, batch):\n",
        "        if isinstance(batch, tuple):\n",
        "            return tuple(item.to(self.device) for item in batch)\n",
        "        return batch.to(self.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-7VUbrw7VTqN"
      },
      "source": [
        "There is a custom node-wise dataloader defined in the following cell. It is provided with the graph `Data` object, containing the node features matrix, the `train`, `test` and `val` masks, etc.\n",
        "\n",
        "* There are also options to specify other parameters.\n",
        "\n",
        "* For our transductive setting, since we do not have any memory constraints to work on our sufficiently small-sized `Facebook` or `Reddit` dataset, we can specify the `batch_size` parameter as `full` to use the entire graph as a single batch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d76kgZBfaatw"
      },
      "outputs": [],
      "source": [
        "from typing import Literal, Optional, Union\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from collections.abc import Iterator\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.utils import k_hop_subgraph, to_edge_index\n",
        "from torch_geometric.transforms import ToSparseTensor\n",
        "\n",
        "\n",
        "class NodeDataLoader:\n",
        "    \"\"\" A fast dataloader for node-wise training.\n",
        "\n",
        "    We have three settings:\n",
        "    1. batch_size = 'full', hops = None\n",
        "        The entire graph is used as a single batch.\n",
        "    2. batch_size = int, hops = None\n",
        "        The entire graph is returned at every iteration with a new phase mask corresponding to the nodes in the batch.\n",
        "    3. batch_size = int, hops = int\n",
        "        The k-hop subgraph is returned at every iteration corresponding to the nodes in the batch.\n",
        "\n",
        "    Args:\n",
        "        data (Data): The graph data object.\n",
        "        subset (LongTensor or BoolTensor, optional): The subset of nodes to use for batching.\n",
        "            If set to None, all nodes are used. (default: None)\n",
        "        batch_size (int or 'full', optional): The batch size.\n",
        "            If set to 'full', the entire graph is used as a single batch.\n",
        "            (default: 'full')\n",
        "        hops (int, optional): The number of hops to sample neighbors.\n",
        "            If set to None, all neighbors are included. (default: None)\n",
        "        shuffle (bool, optional): If set to True, the nodes are shuffled\n",
        "            before batching. (default: True)\n",
        "        drop_last (bool, optional): If set to True, the last batch is\n",
        "            dropped if it is smaller than the batch size. (default: False)\n",
        "        poisson_sampling (bool, optional): If set to True, poisson sampling\n",
        "            is used to sample nodes. (default: False)\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 data: Data,\n",
        "                 subset: Optional[Tensor] = None,\n",
        "                 batch_size: Union[int, Literal['full']] = 'full',\n",
        "                 hops: Optional[int] = None,\n",
        "                 shuffle: bool = True,\n",
        "                 drop_last: bool = False,\n",
        "                 poisson_sampling: bool = False):\n",
        "\n",
        "        self.data = Data(**data.to_dict())\n",
        "        self.batch_size = batch_size\n",
        "        self.hops = hops\n",
        "        self.shuffle = shuffle\n",
        "        self.drop_last = drop_last\n",
        "        self.poisson_sampling = poisson_sampling\n",
        "        self.device = data.x.device\n",
        "\n",
        "        if subset is None:\n",
        "            self.node_indices = torch.arange(data.num_nodes, device=self.device)\n",
        "        else:\n",
        "            if subset.dtype == torch.bool:\n",
        "                self.node_indices = subset.nonzero().view(-1)\n",
        "            else:\n",
        "                self.node_indices = subset\n",
        "\n",
        "        self.num_nodes = self.node_indices.size(0)\n",
        "        self.is_sparse = hasattr(data, 'adj_t')\n",
        "\n",
        "    def __iter__(self) -> Iterator[Data]:\n",
        "        if self.batch_size == 'full':\n",
        "            self.data.batch_nodes = self.node_indices\n",
        "            yield self.data\n",
        "            return\n",
        "\n",
        "        if self.shuffle and not self.poisson_sampling:\n",
        "            perm = torch.randperm(self.num_nodes, device=self.device)\n",
        "            self.node_indices = self.node_indices[perm]\n",
        "\n",
        "        for i in range(0, self.num_nodes, self.batch_size):\n",
        "            if self.drop_last and i + self.batch_size > self.num_nodes:\n",
        "                break\n",
        "\n",
        "            if self.poisson_sampling:\n",
        "                sampling_prob = self.batch_size / self.num_nodes\n",
        "                sample_mask = torch.rand(self.num_nodes, device=self.device) < sampling_prob\n",
        "                batch_nodes = self.node_indices[sample_mask]\n",
        "            else:\n",
        "                batch_nodes = self.node_indices[i:i + self.batch_size]\n",
        "\n",
        "            if self.hops is None:\n",
        "                data = self.data\n",
        "                data.batch_nodes = batch_nodes\n",
        "            else:\n",
        "                if not hasattr(self, 'edge_index'):\n",
        "                    if self.is_sparse:\n",
        "                        self.edge_index, _ = to_edge_index(self.data.adj_t)\n",
        "                    else:\n",
        "                        self.edge_index = self.data.edge_index\n",
        "\n",
        "                subset, batch_edge_index, mapping, _ = k_hop_subgraph(\n",
        "                    node_idx=batch_nodes,\n",
        "                    num_hops=self.hops,\n",
        "                    edge_index=self.edge_index,\n",
        "                    relabel_nodes=True,\n",
        "                    num_nodes=self.data.num_nodes\n",
        "                )\n",
        "\n",
        "                data = Data(\n",
        "                    x=self.data.x[subset],\n",
        "                    y=self.data.y[subset],\n",
        "                    edge_index=batch_edge_index,\n",
        "                    batch_nodes=mapping,\n",
        "                )\n",
        "\n",
        "                if self.is_sparse:\n",
        "                    data = ToSparseTensor(layout=torch.sparse_csr)(data)\n",
        "\n",
        "            yield data\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        if self.batch_size == 'full':\n",
        "            return 1\n",
        "        elif self.drop_last:\n",
        "            return self.num_nodes // self.batch_size\n",
        "        else:\n",
        "            return (self.num_nodes + self.batch_size - 1) // self.batch_size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAYZge1sb5Ww"
      },
      "source": [
        "Now that the trainer and the model-interaction module, which are self-sufficient are defined, we define an abstract class that performs node-level classification on all graph data structures. This will automatically call upon `NodeDataLoader` that accesses the `train_mask`, 'val_mask` etc. to perform training in a node-wise manner and transductive setting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cogfzY_TVS-b"
      },
      "outputs": [],
      "source": [
        "from abc import ABC, abstractmethod\n",
        "from typing import Annotated, Literal, Union\n",
        "from torch import Tensor\n",
        "from torch_geometric.data import Data\n",
        "#from core.data.loader.node import NodeDataLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class NodeClassification(ABC):\n",
        "    def __init__(self,\n",
        "                 num_classes:      int,\n",
        "                 batch_size:       Annotated[Union[Literal['full'], int],\n",
        "                                                    ArgInfo(help='batch size, or \"full\" for full-batch training')] = 'full',\n",
        "                 full_batch_eval:  Annotated[bool,  ArgInfo(help='if true, then model uses full-batch evaluation')] = True,\n",
        "                 **trainer_args:   Annotated[dict,  ArgInfo(help='extra options passed to the trainer class', bases=[Trainer])]\n",
        "                 ):\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.batch_size = batch_size\n",
        "        self.full_batch_eval = full_batch_eval\n",
        "        self.trainer_args = trainer_args\n",
        "\n",
        "        self.data = None\n",
        "        self.classifier = self.configure_classifier()\n",
        "        self.trainer = self.configure_trainer()\n",
        "\n",
        "    def reset(self):\n",
        "        self.classifier.reset_parameters()\n",
        "        self.trainer = self.configure_trainer()\n",
        "        self.data = None\n",
        "\n",
        "    @abstractmethod\n",
        "    def configure_classifier(self) -> TrainableModule:\n",
        "        \"\"\"Configure the classifier module.\"\"\"\n",
        "\n",
        "    def configure_trainer(self) -> Trainer:\n",
        "        \"\"\"Configure the trainer\"\"\"\n",
        "        trainer = Trainer(**self.trainer_args)\n",
        "        return trainer\n",
        "\n",
        "    def setup(self, data: Data) -> None:\n",
        "        \"\"\"Setup method for the given data.\"\"\"\n",
        "        # with console.status('moving data to device'):\n",
        "        #     data = self.to_device(data)\n",
        "\n",
        "        self.data = Data(**data.to_dict())\n",
        "\n",
        "    def run(self, data: Data, fit: bool = True, test: bool = True) -> Metrics:\n",
        "        \"\"\"Setup the model for the given data, and run the training and testing procedures.\"\"\"\n",
        "        metrics = {}\n",
        "        self.setup(data)\n",
        "\n",
        "        if fit:\n",
        "            train_metrics = self.fit()\n",
        "            metrics.update(train_metrics)\n",
        "        if test:\n",
        "            test_metrics = self.test()\n",
        "            metrics.update(test_metrics)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def fit(self) -> Metrics:\n",
        "        \"\"\"Fit the method to the given data.\"\"\"\n",
        "        metrics = self.trainer.fit(\n",
        "            model=self.classifier,\n",
        "            train_dataloader=self.data_loader('train'),\n",
        "            val_dataloader=self.data_loader('val'),\n",
        "            test_dataloader=self.data_loader('test')\n",
        "        )\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def test(self) -> Metrics:\n",
        "        \"\"\"Test the method on the given data.\"\"\"\n",
        "        return self.trainer.test(\n",
        "            dataloader=self.data_loader('test')\n",
        "        )\n",
        "\n",
        "    def predict(self) -> Tensor:\n",
        "        \"\"\"Predict output for the given data.\"\"\"\n",
        "        return self.trainer.predict(\n",
        "            dataloader=self.data_loader('predict'),\n",
        "        )\n",
        "\n",
        "    def data_loader(self, phase: Phase) -> NodeDataLoader:\n",
        "        \"\"\"Return a dataloader for the given phase.\"\"\"\n",
        "        batch_size = 'full' if (phase != 'train' and self.full_batch_eval) else self.batch_size\n",
        "        subset = self.data[f'{phase}_mask'] if phase != 'predict' else None\n",
        "        shuffle = phase == 'train'\n",
        "\n",
        "        dataloader = NodeDataLoader(\n",
        "            data=self.data,\n",
        "            subset=subset,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=shuffle,\n",
        "            drop_last=phase == 'train',\n",
        "            poisson_sampling=False,\n",
        "        )\n",
        "\n",
        "        return dataloader\n",
        "\n",
        "    def to_device(self, data: Union[Data, Tensor]):\n",
        "        \"\"\"Move the data to the device.\"\"\"\n",
        "        return self.trainer.to_device(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZJKr35adUy6"
      },
      "source": [
        "### NON-PRIVATE NODE CLASSIFICATION METHOD : 1 (Multi-Layer Perceptron)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCJLrfA-dnGc"
      },
      "outputs": [],
      "source": [
        "from typing import Callable\n",
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import Dropout, ModuleList, BatchNorm1d, Module\n",
        "from torch_geometric.nn import Linear\n",
        "from opacus.grad_sample import register_grad_sampler, compute_linear_grad_sample\n",
        "\n",
        "\n",
        "@register_grad_sampler(Linear)\n",
        "def compute_lazy_linear_grad_sample(layer, activations, backprops):\n",
        "    return compute_linear_grad_sample(layer, activations, backprops)\n",
        "\n",
        "\n",
        "class MLP(Module):\n",
        "    \"\"\"\n",
        "    A multi-layer perceptron (MLP) model.\n",
        "    This implementation handles 0-layer configurations as well.\n",
        "    \"\"\"\n",
        "    def __init__(self, *,\n",
        "                 output_dim: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 num_layers: int = 2,\n",
        "                 dropout: float = 0.0,\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 batch_norm: bool = False,\n",
        "                 plain_last: bool = True,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.dropout_fn = Dropout(dropout, inplace=True)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.plain_last = plain_last\n",
        "\n",
        "        dimensions = [hidden_dim] * (num_layers - 1) + [output_dim] * (num_layers > 0)\n",
        "        self.layers: list[Linear] = ModuleList([Linear(-1, dim) for dim in dimensions])\n",
        "\n",
        "        self.bns: list[BatchNorm1d] = []\n",
        "        if batch_norm:\n",
        "            self.bns = ModuleList([BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)])\n",
        "            if not plain_last:\n",
        "                self.bns.append(BatchNorm1d(output_dim))\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            x = layer(x)\n",
        "            if i < self.num_layers - self.plain_last:\n",
        "                x = self.activation_fn(x)\n",
        "                x = self.bns[i](x) if self.bns else x\n",
        "                x = self.dropout_fn(x)\n",
        "        return x\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for layer in self.layers:\n",
        "            layer.reset_parameters()\n",
        "\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "shuBpDghVmSx"
      },
      "outputs": [],
      "source": [
        "from typing import Callable, Optional\n",
        "import torch\n",
        "from torch import Tensor\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "class MLPNodeClassifier(TrainableModule):\n",
        "    def __init__(self, *,\n",
        "                 num_classes: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 num_layers: int = 2,\n",
        "                 dropout: float = 0.0,\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 batch_norm: bool = False,\n",
        "                 **kwargs,\n",
        "                 ):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.model = MLP(\n",
        "            output_dim=num_classes,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            activation_fn=activation_fn,\n",
        "            batch_norm=batch_norm,\n",
        "            plain_last=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        return self.model(x)\n",
        "\n",
        "    def step(self, data: Data, phase: Phase) -> tuple[Optional[Tensor], Metrics]:\n",
        "        x, y = data.x[data.batch_nodes], data.y[data.batch_nodes]\n",
        "        preds = F.log_softmax(self(x), dim=-1)\n",
        "        acc = preds.detach().argmax(dim=1).eq(y).float().mean() * 100\n",
        "        metrics = {f'{phase}/acc': acc}\n",
        "\n",
        "        loss = None\n",
        "        if phase != 'test':\n",
        "            loss = F.nll_loss(input=preds, target=y)\n",
        "            metrics[f'{phase}/loss'] = loss.detach()\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def predict(self, data: Data) -> Tensor:\n",
        "        h = self(data.x[data.batch_nodes])\n",
        "        return torch.softmax(h, dim=-1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        return self.model.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W789AqgWfZQz",
        "outputId": "48f61500-16c7-4a8a-f124-725e62a229b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data(x=[1120280, 537], y=[1120280], train_mask=[1120280], val_mask=[1120280], test_mask=[1120280], adj_t=[1120280, 1120280])\n"
          ]
        }
      ],
      "source": [
        "print(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCogrXzxfdgK",
        "outputId": "6ea543b2-0e8a-4658-ed9a-88865144810d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(5)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.y.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUiLSrOjgRg6",
        "outputId": "ec09bdc1-35e0-422a-c9a3-835fb7afa109",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 386
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'class_resolver'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-cd09abc563a4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAnnotated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mclass_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivation_resolver\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSimpleMLP\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mNodeClassification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m\"\"\"Non-private MLP method\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'class_resolver'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from typing import Annotated\n",
        "from class_resolver.contrib.torch import activation_resolver\n",
        "\n",
        "class SimpleMLP (NodeClassification):\n",
        "    \"\"\"Non-private MLP method\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 hidden_dim:      Annotated[int,   ArgInfo(help='dimension of the hidden layers')] = 16,\n",
        "                 num_layers:      Annotated[int,   ArgInfo(help='number of MLP layers')] = 2,\n",
        "                 activation:      Annotated[str,   ArgInfo(help='type of activation function', choices=['relu', 'selu', 'tanh'])] = 'selu',\n",
        "                 dropout:         Annotated[float, ArgInfo(help='dropout rate')] = 0.0,\n",
        "                 batch_norm:      Annotated[bool,  ArgInfo(help='if true, then model uses batch normalization')] = True,\n",
        "                 optimizer:       Annotated[str,   ArgInfo(help='optimization algorithm', choices=['sgd', 'adam'])] = 'adam',\n",
        "                 learning_rate:   Annotated[float, ArgInfo(help='learning rate', option='--lr')] = 0.01,\n",
        "                 weight_decay:    Annotated[float, ArgInfo(help='weight decay (L2 penalty)')] = 0.0,\n",
        "                 **kwargs:        Annotated[dict,  ArgInfo(help='extra options passed to base class', bases=[NodeClassification])]\n",
        "                 ):\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_layers = num_layers\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm = batch_norm\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        super().__init__(num_classes, **kwargs)\n",
        "\n",
        "    def configure_classifier(self) -> TrainableModule:\n",
        "        return MLPNodeClassifier(\n",
        "            num_classes=self.num_classes,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            num_layers=self.num_layers,\n",
        "            activation_fn=activation_resolver.make(self.activation),\n",
        "            dropout=self.dropout,\n",
        "            batch_norm=self.batch_norm,\n",
        "            optimizer=self.optimizer,\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28UWYVapidiF"
      },
      "outputs": [],
      "source": [
        "#--dataset facebook --hidden_dim 16 --activation selu --optimizer adam --learning_rate 0.01 --repeats 10 --epochs 100 --batch_size full --verbose False --base_layers 1 --head_layers 1 --jk cat --depth 1 --layerwise False --logger wandb --project ProGAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "e629fa88b5f043a6aa6d935d5da48ba8",
            "7de337c84a3640a68c3c50c859b8a780"
          ]
        },
        "id": "IBfbimLag77k",
        "outputId": "f27f78f2-131f-4c39-f0b3-dc3ed8598ffc"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e629fa88b5f043a6aa6d935d5da48ba8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 79, 'train/acc': tensor(35.4878), 'train/loss': tensor(1.4758), 'val/acc': tensor(34.6386), 'val/loss': tensor(1.4829), 'test/acc': tensor(34.9495)}\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the SimpleMLP model with the desired number of classes and other parameters\n",
        "num_classes = data.y.max() + 1\n",
        "model = SimpleMLP(num_classes= num_classes, hidden_dim=16, num_layers=2, activation='selu', dropout=0.0, batch_norm=True, optimizer='adam', learning_rate=0.01, weight_decay=0.0)\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_base_mlp = model.run(data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_base_mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5rMnCSDjFsMu"
      },
      "source": [
        "#### Running baseline model for Reddit data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "1099b26d1696436a9a48017562acad8d",
            "d093526d7989486d9500b32fb6bf6ad8"
          ]
        },
        "id": "p4AWu_wNEpNq",
        "outputId": "75b4dfe0-fa61-4c8a-96d2-28ff088f591e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1099b26d1696436a9a48017562acad8d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 97, 'train/acc': tensor(84.0992), 'train/loss': tensor(0.4731), 'val/acc': tensor(81.9710), 'val/loss': tensor(0.5501), 'test/acc': tensor(82.2602)}\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the SimpleMLP model with the desired number of classes and other parameters\n",
        "num_classes_reddit = reddit_data.y.max() + 1\n",
        "model = SimpleMLP(num_classes= num_classes_reddit, hidden_dim=16, num_layers=2, activation='selu', dropout=0.0, batch_norm=True, optimizer='adam', learning_rate=0.01, weight_decay=0.0)\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_base_mlp_reddit = model.run(reddit_data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_base_mlp_reddit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGl8YophqcI5"
      },
      "source": [
        "Now that we have the first baseline, let's establish the second baseline for non-private setting.\n",
        "\n",
        "NON-PRIVATE NODE CLASSIFICATION METHOD : 2 (GraphSAGE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wy8MxMJFiuIU"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.nn import GraphSAGE, GCN, GAT\n",
        "\n",
        "\n",
        "class GNN(Module):\n",
        "    \"\"\"\n",
        "    A flexible GNN model.\n",
        "    This implementation supports plain_last option.\n",
        "    \"\"\"\n",
        "    def __init__(self, *,\n",
        "                 conv: str,\n",
        "                 output_dim: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 num_layers: int = 2,\n",
        "                 dropout: float = 0.0,\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 batch_norm: bool = False,\n",
        "                 jk: str = None,\n",
        "                 plain_last: bool = True,\n",
        "                 **conv_kwargs\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        model_kwargs = dict(\n",
        "            in_channels=-1,\n",
        "            hidden_channels=hidden_dim,\n",
        "            out_channels=output_dim,\n",
        "            num_layers=num_layers,\n",
        "            act=activation_fn,\n",
        "            dropout=dropout,\n",
        "            norm='batchnorm' if batch_norm else None,\n",
        "            jk=jk,\n",
        "        )\n",
        "\n",
        "        if conv == 'sage':\n",
        "            self.model = GraphSAGE(\n",
        "                project=False,\n",
        "                **model_kwargs,\n",
        "                **conv_kwargs,\n",
        "            )\n",
        "        elif conv == 'gcn':\n",
        "            self.model = GCN(\n",
        "                **model_kwargs,\n",
        "                **conv_kwargs,\n",
        "            )\n",
        "        elif conv == 'gat':\n",
        "            self.model = GAT(\n",
        "                **model_kwargs,\n",
        "                **conv_kwargs,\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unknown conv type: {conv}')\n",
        "\n",
        "        self.dropout_fn = Dropout(dropout, inplace=True)\n",
        "        self.activation_fn = activation_fn\n",
        "        self.batch_norm = batch_norm\n",
        "        self.plain_last = plain_last\n",
        "        if not plain_last and batch_norm:\n",
        "            self.bn = BatchNorm1d(output_dim)\n",
        "\n",
        "    def forward(self, x: Tensor, adj_t: Tensor) -> Tensor:\n",
        "        x = self.model(x, adj_t)\n",
        "        if not self.plain_last:\n",
        "            x = self.bn(x) if self.batch_norm else x\n",
        "            x = self.dropout_fn(x)\n",
        "            x = self.activation_fn(x)\n",
        "        return x\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.model.reset_parameters()\n",
        "        if hasattr(self, 'bn'):\n",
        "            self.bn.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mil3OjCdsoF6"
      },
      "outputs": [],
      "source": [
        "class MultiMLP(Module):\n",
        "    CombType = Literal['cat', 'sum', 'max', 'mean', 'att']\n",
        "    supported_combinations = get_args(CombType)\n",
        "\n",
        "    def __init__(self, *,\n",
        "                 num_channels: int,\n",
        "                 output_dim: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 base_layers: int = 2,\n",
        "                 head_layers: int = 1,\n",
        "                 combination: CombType = 'cat',\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 dropout: float = 0.0,\n",
        "                 batch_norm: bool = False,\n",
        "                 plain_last: bool = True,\n",
        "                 ):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.combination = combination\n",
        "        self.activation_fn = activation_fn\n",
        "        self.dropout_fn = Dropout(dropout, inplace=True)\n",
        "\n",
        "        self.base_mlps: list[MLP] = ModuleList([\n",
        "            MLP(\n",
        "                hidden_dim=hidden_dim,\n",
        "                output_dim=hidden_dim,\n",
        "                num_layers=base_layers,\n",
        "                dropout=dropout,\n",
        "                activation_fn=activation_fn,\n",
        "                batch_norm=batch_norm,\n",
        "                plain_last=True,\n",
        "            ) for _ in range(num_channels)]\n",
        "        )\n",
        "\n",
        "        if combination == 'att':\n",
        "            self.hidden_dim = hidden_dim\n",
        "            self.num_heads = num_channels\n",
        "            self.Q = Linear(in_features=hidden_dim, out_features=self.num_heads, bias=False)\n",
        "\n",
        "        self.bn = LazyBatchNorm1d() if batch_norm else False\n",
        "\n",
        "        self.head_mlp = MLP(\n",
        "            output_dim=output_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=head_layers,\n",
        "            dropout=dropout,\n",
        "            activation_fn=activation_fn,\n",
        "            batch_norm=batch_norm,\n",
        "            plain_last=plain_last,\n",
        "        )\n",
        "\n",
        "    def forward(self, x_stack: Tensor) -> Tensor:\n",
        "        x_stack = x_stack.permute(2, 0, 1) # (hop, node, input_dim)\n",
        "        h_list = [mlp(x) for x, mlp in zip(x_stack, self.base_mlps)]\n",
        "        h = self.combine(h_list)\n",
        "        h = F.normalize(h, p=2, dim=-1)\n",
        "        h = self.bn(h) if self.bn else h\n",
        "        h = self.dropout_fn(h)\n",
        "        h = self.activation_fn(h)\n",
        "        h = self.head_mlp(h)\n",
        "        return h\n",
        "\n",
        "    def combine(self, h_list: Iterable[Tensor]) -> Tensor:\n",
        "        if self.combination == 'cat':\n",
        "            return torch.cat(h_list, dim=-1)\n",
        "        elif self.combination == 'sum':\n",
        "            return torch.stack(h_list, dim=0).sum(dim=0)\n",
        "        elif self.combination == 'mean':\n",
        "            return torch.stack(h_list, dim=0).mean(dim=0)\n",
        "        elif self.combination == 'max':\n",
        "            return torch.stack(h_list, dim=0).max(dim=0).values\n",
        "        elif self.combination == 'att':\n",
        "            H = torch.stack(h_list, dim=1)  # (node, hop, dim)\n",
        "            W = F.leaky_relu(self.Q(H), 0.2).softmax(dim=0)  # (node, hop, head)\n",
        "            out = H.transpose(1, 2).matmul(W).view(-1, self.hidden_dim * self.num_heads)\n",
        "            return out\n",
        "        else:\n",
        "            raise ValueError(f'Unknown combination type {self.combination}')\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for mlp in self.base_mlps: mlp.reset_parameters()\n",
        "        if self.combination == 'att':\n",
        "            self.Q.reset_parameters()\n",
        "        if self.bn: self.bn.reset_parameters()\n",
        "        self.head_mlp.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjui5C0vte0V"
      },
      "outputs": [],
      "source": [
        "from torch.nn import Module, MultiheadAttention\n",
        "from torch_geometric.nn import JumpingKnowledge as JK, Linear\n",
        "\n",
        "\n",
        "class SelfAttention(MultiheadAttention):\n",
        "    def forward(self, xs: Tensor) -> Tensor:\n",
        "        \"\"\"forward propagation\n",
        "\n",
        "        Args:\n",
        "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: output tensor with size (num_nodes, hidden_dim)\n",
        "        \"\"\"\n",
        "        x = xs.transpose(2, int(self.batch_first))\n",
        "        out: Tensor = super().forward(x, x, x, need_weights=True)[0]\n",
        "        return out.mean(dim=int(self.batch_first))\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        super()._reset_parameters()\n",
        "\n",
        "\n",
        "class WeightedSum(Module):\n",
        "    def __init__(self, hidden_dim: int, num_heads: int):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.Q = Linear(in_channels=hidden_dim, out_channels=num_heads, bias=False)\n",
        "\n",
        "        if num_heads > 1:\n",
        "            self.fc = Linear(in_channels=num_heads, out_channels=1, bias=False)\n",
        "\n",
        "    def forward(self, xs: Tensor) -> Tensor:\n",
        "        \"\"\"forward propagation\n",
        "\n",
        "        Args:\n",
        "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: output tensor with size (num_nodes, hidden_dim)\n",
        "        \"\"\"\n",
        "        H = xs.transpose(1, 2)  # (node, hop, dim)\n",
        "        W = self.Q(H).softmax(dim=1)  # (node, hop, head)\n",
        "        Z = H.transpose(1, 2).matmul(W)\n",
        "\n",
        "        if self.num_heads > 1:\n",
        "            Z = self.fc(Z)\n",
        "\n",
        "        return Z.squeeze(-1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.Q.reset_parameters()\n",
        "        if self.num_heads > 1:\n",
        "            self.fc.reset_parameters()\n",
        "\n",
        "\n",
        "class JumpingKnowledge(Module):\n",
        "    supported_modes = ['cat', 'max', 'lstm', 'sum', 'mean', 'attn', 'wsum']\n",
        "    def __init__(self, mode: str, **kwargs):\n",
        "        super().__init__()\n",
        "        self.mode = mode\n",
        "        if mode == 'attn':\n",
        "            self.hidden_dim = kwargs['hidden_dim']\n",
        "            self.num_heads = kwargs['num_heads']\n",
        "            self.attn = SelfAttention(self.hidden_dim, num_heads=self.num_heads, batch_first=True)\n",
        "        elif mode == 'wsum':\n",
        "            self.hidden_dim = kwargs['hidden_dim']\n",
        "            self.num_heads = kwargs['num_heads']\n",
        "            self.wsum = WeightedSum(self.hidden_dim, num_heads=self.num_heads)\n",
        "        elif mode == 'lstm':\n",
        "            self.lstm = JK(mode='lstm', **kwargs)\n",
        "\n",
        "    def forward(self, xs: Tensor) -> Tensor:\n",
        "        \"\"\"forward propagation\n",
        "\n",
        "        Args:\n",
        "            xs (Tensor): input with shape (batch_size, hidden_dim, num_phases)\n",
        "\n",
        "        Returns:\n",
        "            Tensor: aggregated output with shape (batch_size, hidden_dim)\n",
        "        \"\"\"\n",
        "        if self.mode == 'cat':\n",
        "            return xs.transpose(1,2).reshape(xs.size(0), -1)\n",
        "        elif self.mode == 'sum':\n",
        "            return xs.sum(dim=-1)\n",
        "        elif self.mode == 'mean':\n",
        "            return xs.mean(dim=-1)\n",
        "        elif self.mode == 'max':\n",
        "            return xs.max(dim=-1)[0]\n",
        "        elif self.mode == 'attn':\n",
        "            return self.attn(xs)\n",
        "        elif self.mode == 'wsum':\n",
        "            return self.wsum(xs)\n",
        "        elif self.mode == 'lstm':\n",
        "            return self.lstm(xs.unbind(dim=-1))\n",
        "        else:\n",
        "            raise NotImplementedError(f'Unsupported JK mode: {self.mode}')\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for module in self.children():\n",
        "            module.reset_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JB1Dh6zbtmE3"
      },
      "source": [
        "he above modules will be used as single layers (part of a larger architecture) in our process of node classification. We ensure learnability and at the same time, ensure epsilon-Differential Privacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv-OiLOVtjM8"
      },
      "outputs": [],
      "source": [
        "class GNNNodeClassifier(TrainableModule):\n",
        "    def __init__(self, *,\n",
        "                 num_classes: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 base_layers: int = 1,\n",
        "                 mp_layers: int = 2,\n",
        "                 head_layers: int = 0,\n",
        "                 conv: str = 'sage',\n",
        "                 conv_kwargs: dict = {},\n",
        "                 jk: str = None,\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 dropout: float = 0.0,\n",
        "                 batch_norm: bool = False,\n",
        "                 **kwargs\n",
        "                 ):\n",
        "\n",
        "        assert mp_layers > 0, 'Must have at least one message passing layer'\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.base_mlp = MLP(\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=hidden_dim,\n",
        "            num_layers=base_layers,\n",
        "            activation_fn=activation_fn,\n",
        "            dropout=dropout,\n",
        "            batch_norm=batch_norm,\n",
        "            plain_last=False,\n",
        "        )\n",
        "# Defining message passing layer (individual)\n",
        "        self.gnn = GNN(\n",
        "            conv=conv,\n",
        "            output_dim=num_classes if head_layers == 0 else hidden_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=mp_layers,\n",
        "            dropout=dropout,\n",
        "            jk=jk,\n",
        "            activation_fn=activation_fn,\n",
        "            batch_norm=batch_norm,\n",
        "            plain_last=head_layers == 0,\n",
        "            **conv_kwargs,\n",
        "        )\n",
        "\n",
        "# Defining classification Head\n",
        "        self.head_mlp = MLP(\n",
        "            hidden_dim=hidden_dim,\n",
        "            output_dim=num_classes,\n",
        "            num_layers=head_layers,\n",
        "            activation_fn=activation_fn,\n",
        "            dropout=dropout,\n",
        "            batch_norm=batch_norm,\n",
        "            plain_last=True,\n",
        "        )\n",
        "\n",
        "    def forward(self, x: Tensor, adj_t: Tensor) -> Tensor:\n",
        "        x = self.base_mlp(x)\n",
        "        x = self.gnn(x, adj_t)\n",
        "        x = self.head_mlp(x)\n",
        "        return x\n",
        "\n",
        "    def step(self, data: Data, phase: Phase) -> tuple[Optional[Tensor], Metrics]:\n",
        "        h = self(data.x, data.adj_t)\n",
        "        h, y = h[data.batch_nodes], data.y[data.batch_nodes]\n",
        "        preds = F.log_softmax(h, dim=-1)\n",
        "        acc = preds.detach().argmax(dim=1).eq(y).float().mean() * 100\n",
        "        metrics = {f'{phase}/acc': acc}\n",
        "\n",
        "        loss = None\n",
        "        if phase != 'test':\n",
        "            loss = F.nll_loss(input=preds, target=y)\n",
        "            metrics[f'{phase}/loss'] = loss.detach()\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def predict(self, data: Data) -> Tensor:\n",
        "        h = self(data.x, data.adj_t)[data.batch_nodes]\n",
        "        return torch.softmax(h, dim=-1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.base_mlp.reset_parameters()\n",
        "        self.gnn.reset_parameters()\n",
        "        self.head_mlp.reset_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDtSEl_its3k"
      },
      "outputs": [],
      "source": [
        "from class_resolver.contrib.torch import activation_resolver\n",
        "class StandardGNN (NodeClassification):\n",
        "    \"\"\"Non-private GNN method\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 conv:                  Annotated[str,   ArgInfo(help='type of convolution layer', choices=['sage', 'gcn', 'gin', 'gat'])] = 'sage',\n",
        "                 hidden_dim:            Annotated[int,   ArgInfo(help='dimension of the hidden layers')] = 16,\n",
        "                 base_layers:           Annotated[int,   ArgInfo(help='number of base MLP layers')] = 1,\n",
        "                 mp_layers:             Annotated[int,   ArgInfo(help='number of message passing layers')] = 2,\n",
        "                 head_layers:           Annotated[int,   ArgInfo(help='number of head MLP layers')] = 1,\n",
        "                 activation:            Annotated[str,   ArgInfo(help='type of activation function', choices=['relu', 'selu', 'tanh'])] = 'selu',\n",
        "                 dropout:               Annotated[float, ArgInfo(help='dropout rate')] = 0.0,\n",
        "                 batch_norm:            Annotated[bool,  ArgInfo(help='if true, then model uses batch normalization')] = True,\n",
        "                 jk:                    Annotated[str,   ArgInfo(help='the jumping knowledge mode.', choices=[\"last\", \"cat\", \"max\", \"lstm\"])] = 'cat',\n",
        "                 # sage args\n",
        "                 sage_aggr:             Annotated[str,   ArgInfo(help='SAGE: type of aggregation function', choices=['mean', 'sum', 'max', 'lstm'])] = 'mean',\n",
        "                 sage_root_weight:      Annotated[bool,  ArgInfo(help='SAGE: if true, will add transformed root node features to the output')] = True,\n",
        "                 sage_normalize:        Annotated[bool,  ArgInfo(help='SAGE: if true, output features will be L2-normalized')] = True,\n",
        "                 # gcn args\n",
        "                 gcn_improved:          Annotated[bool,  ArgInfo(help='GCN: if true, will use improved version')] = False,\n",
        "                 gcn_add_self_loops:    Annotated[bool,  ArgInfo(help='GCN: if true, will add self loops to the adjacency matrix')] = True,\n",
        "                 gcn_cached:            Annotated[bool,  ArgInfo(help='GCN: if true, will cache the normalized adjacency matrix')] = True,\n",
        "                 gcn_normalize:         Annotated[bool,  ArgInfo(help='GCN: whether to add self-loops and compute symmetric normalization coefficients on the fly')] = True,\n",
        "                 # gat args\n",
        "                 gat_heads:             Annotated[int,   ArgInfo(help='GAT: number of attention heads')] = 1,\n",
        "                 gat_concat:            Annotated[bool,  ArgInfo(help='GAT: if true, will concatenate multi-head attentions; otherwise, will average them')] = True,\n",
        "                 gat_negative_slope:    Annotated[float, ArgInfo(help='GAT: negative slope of the leaky relu activation')] = 0.2,\n",
        "                 gat_add_self_loops:    Annotated[bool,  ArgInfo(help='GAT: if true, will add self loops to the adjacency matrix')] = True,\n",
        "                 # optimizer args\n",
        "                 optimizer:             Annotated[str,   ArgInfo(help='optimization algorithm', choices=['sgd', 'adam'])] = 'adam',\n",
        "                 learning_rate:         Annotated[float, ArgInfo(help='learning rate', option='--lr')] = 0.01,\n",
        "                 weight_decay:          Annotated[float, ArgInfo(help='weight decay (L2 penalty)')] = 0.0,\n",
        "                 **kwargs:              Annotated[dict,  ArgInfo(help='extra options passed to base class', bases=[NodeClassification])]\n",
        "                 ):\n",
        "\n",
        "        assert mp_layers >= 1, 'number of message-passing layers must be at least 1'\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.conv = conv\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.base_layers = base_layers\n",
        "        self.mp_layers = mp_layers\n",
        "        self.head_layers = head_layers\n",
        "        self.activation_fn = activation_resolver.make(activation)\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm = batch_norm\n",
        "        self.jk = jk\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        if self.conv == 'sage':\n",
        "            self.conv_kwargs = {\n",
        "                'aggr': sage_aggr,\n",
        "                'root_weight': sage_root_weight,\n",
        "                'normalize': sage_normalize,\n",
        "            }\n",
        "        elif self.conv == 'gcn':\n",
        "            self.conv_kwargs = {\n",
        "                'improved': gcn_improved,\n",
        "                'add_self_loops': gcn_add_self_loops,\n",
        "                'cached': gcn_cached,\n",
        "                'normalize': gcn_normalize,\n",
        "            }\n",
        "        elif self.conv == 'gat':\n",
        "            self.conv_kwargs = {\n",
        "                'heads': gat_heads,\n",
        "                'concat': gat_concat,\n",
        "                'negative_slope': gat_negative_slope,\n",
        "                'add_self_loops': gat_add_self_loops,\n",
        "            }\n",
        "\n",
        "        super().__init__(num_classes, **kwargs)\n",
        "\n",
        "    def configure_classifier(self) -> TrainableModule:\n",
        "        return GNNNodeClassifier(\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            num_classes=self.num_classes,\n",
        "            base_layers=self.base_layers,\n",
        "            mp_layers=self.mp_layers,\n",
        "            head_layers=self.head_layers,\n",
        "            conv=self.conv,\n",
        "            conv_kwargs=self.conv_kwargs,\n",
        "            activation_fn=self.activation_fn,\n",
        "            dropout=self.dropout,\n",
        "            batch_norm=self.batch_norm,\n",
        "            jk=self.jk,\n",
        "            optimizer=self.optimizer,\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AHnPm6chu7kX"
      },
      "outputs": [],
      "source": [
        "#pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "NUdHFtFTuKVJ",
        "outputId": "5aca260b-4a20-4a15-bc0b-4cf51e250cb9"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:26:23</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">60.425</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.039</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">58.013</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.080</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">57.961</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠼\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:26:23\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m60.425\u001b[0m train/loss: \u001b[1;36m1.039\u001b[0m  val/acc: \u001b[1;36m58.013\u001b[0m val/loss: \u001b[1;36m1.080\u001b[0m  test/acc: \u001b[1;36m57.961\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 93, 'train/acc': tensor(60.3749), 'train/loss': tensor(1.0420), 'val/acc': tensor(58.7972), 'val/loss': tensor(1.0680), 'test/acc': tensor(58.8727)}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Instantiate the StandardGNN model with the desired parameters\n",
        "model = StandardGNN(\n",
        "    num_classes=num_classes, # Assuming 'data.y' contains the class labels\n",
        "    conv='sage', # Convolution type\n",
        "    hidden_dim=16, # Dimension of the hidden layers\n",
        "    base_layers=1, # Number of base MLP layers\n",
        "    mp_layers=1, # Number of message passing layers\n",
        "    head_layers=1, # Number of head MLP layers\n",
        "    activation='selu', # Activation function\n",
        "    dropout=0.0, # Dropout rate\n",
        "    batch_norm=True, # Batch normalization\n",
        "    jk='cat', # Jumping knowledge mode\n",
        "    sage_aggr='mean', # SAGE aggregation function\n",
        "    sage_root_weight=True, # SAGE root weight\n",
        "    sage_normalize=True, # SAGE normalization\n",
        "    gcn_improved=False, # GCN improved version\n",
        "    gcn_add_self_loops=True, # GCN add self loops\n",
        "    gcn_cached=True, # GCN caching\n",
        "    gcn_normalize=True, # GCN normalization\n",
        "    gat_heads=1, # GAT number of heads\n",
        "    gat_concat=True, # GAT concatenation\n",
        "    gat_negative_slope=0.2, # GAT negative slope\n",
        "    gat_add_self_loops=True, # GAT add self loops\n",
        "    optimizer='adam', # Optimization algorithm\n",
        "    learning_rate=0.01, # Learning rate\n",
        "    weight_decay=0.0 # Weight decay\n",
        ")\n",
        "\n",
        "# Instantiate the Logger\n",
        "# logger = Logger(\n",
        "#     logger='wandb', # Logger type\n",
        "#     project='ProGAP', # Project name for logger\n",
        "#     output_dir='./output', # Directory to store the results\n",
        "#     config={} # Additional configuration for the logger\n",
        "# )\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_base_gnn = model.run(data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_base_gnn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XgV8Q0pF_Ms"
      },
      "source": [
        "#### Running baseline GNN model for Reddit data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "r4Iq3jtMF5Hk",
        "outputId": "3456d9bf-312d-4360-b6ca-5777963368fd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠹</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:18:17</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.299</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.028</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.129</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.035</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.097</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠹\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:18:17\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m99.299\u001b[0m train/loss: \u001b[1;36m0.028\u001b[0m  val/acc: \u001b[1;36m99.129\u001b[0m val/loss: \u001b[1;36m0.035\u001b[0m  test/acc: \u001b[1;36m99.097\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 100, 'train/acc': tensor(99.3072), 'train/loss': tensor(0.0278), 'val/acc': tensor(99.1378), 'val/loss': tensor(0.0353), 'test/acc': tensor(99.0973)}\n"
          ]
        }
      ],
      "source": [
        "model = StandardGNN(\n",
        "    num_classes=num_classes_reddit, # Assuming 'data.y' contains the class labels\n",
        "    conv='sage', # Convolution type\n",
        "    hidden_dim=16, # Dimension of the hidden layers\n",
        "    base_layers=1, # Number of base MLP layers\n",
        "    mp_layers=1, # Number of message passing layers\n",
        "    head_layers=1, # Number of head MLP layers\n",
        "    activation='selu', # Activation function\n",
        "    dropout=0.0, # Dropout rate\n",
        "    batch_norm=True, # Batch normalization\n",
        "    jk='cat', # Jumping knowledge mode\n",
        "    sage_aggr='mean', # SAGE aggregation function\n",
        "    sage_root_weight=True, # SAGE root weight\n",
        "    sage_normalize=True, # SAGE normalization\n",
        "    gcn_improved=False, # GCN improved version\n",
        "    gcn_add_self_loops=True, # GCN add self loops\n",
        "    gcn_cached=True, # GCN caching\n",
        "    gcn_normalize=True, # GCN normalization\n",
        "    gat_heads=1, # GAT number of heads\n",
        "    gat_concat=True, # GAT concatenation\n",
        "    gat_negative_slope=0.2, # GAT negative slope\n",
        "    gat_add_self_loops=True, # GAT add self loops\n",
        "    optimizer='adam', # Optimization algorithm\n",
        "    learning_rate=0.01, # Learning rate\n",
        "    weight_decay=0.0 # Weight decay\n",
        ")\n",
        "\n",
        "# Instantiate the Logger\n",
        "# logger = Logger(\n",
        "#     logger='wandb', # Logger type\n",
        "#     project='ProGAP', # Project name for logger\n",
        "#     output_dir='./output', # Directory to store the results\n",
        "#     config={} # Additional configuration for the logger\n",
        "# )\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_base_gnn_reddit = model.run(reddit_data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_base_gnn_reddit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gDUh8omwy5oK",
        "outputId": "50da73d4-bf61-4b43-de33-f46b26378916"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting autodp\n",
            "  Downloading autodp-0.2.3.1.tar.gz (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from autodp) (1.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autodp) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from autodp) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autodp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->autodp) (1.16.0)\n",
            "Building wheels for collected packages: autodp\n",
            "  Building wheel for autodp (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autodp: filename=autodp-0.2.3.1-py3-none-any.whl size=59817 sha256=bea19ad7ad531fa26c78301d6a1ad02b46d89990ca68289d812a07c0f84fa91c\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/fa/09/a4bf92616f2b3353c47e75d36c14afc2aefce2b78ad6e3536f\n",
            "Successfully built autodp\n",
            "Installing collected packages: autodp\n",
            "Successfully installed autodp-0.2.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autodp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wbXY5NCZy549"
      },
      "outputs": [],
      "source": [
        "from typing_extensions import Self\n",
        "from scipy.optimize import minimize_scalar, OptimizeResult\n",
        "from autodp.mechanism_zoo import Mechanism\n",
        "import numpy as np\n",
        "from autodp.mechanism_zoo import ExactGaussianMechanism\n",
        "\n",
        "class ZeroMechanism(Mechanism):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'ZeroMechanism'\n",
        "        self.params = {}\n",
        "        self.propagate_updates(func=lambda _: 0, type_of_update='RDP')\n",
        "\n",
        "\n",
        "class InfMechanism(Mechanism):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.name = 'InfMechanism'\n",
        "        self.params = {}\n",
        "\n",
        "\n",
        "\n",
        "class NoisyMechanism(Mechanism):\n",
        "    def __init__(self, noise_scale: float):\n",
        "        # \"noise_scale\" is the std of the noise divide by the sensitivity\n",
        "        super().__init__()\n",
        "        self.name = 'NoisyMechanism'\n",
        "        self.params = {'noise_scale': noise_scale}\n",
        "        self.console = Console()\n",
        "\n",
        "    def update(self, noise_scale: float) -> Self:\n",
        "        self.params.pop('noise_scale')\n",
        "        self.__init__(noise_scale, **self.params)\n",
        "        return self\n",
        "\n",
        "    def is_zero(self) -> bool:\n",
        "        for alpha in range(2,100):\n",
        "            if self.RenyiDP(alpha) > 0:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def is_inf(self) -> bool:\n",
        "        for alpha in range(2,100):\n",
        "            if not np.isinf(self.RenyiDP(alpha)):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def calibrate(self, eps: float, delta: float) -> float:\n",
        "        if self.params['noise_scale'] == 0:\n",
        "            self.update(noise_scale=1)  # to avoid is_inf being true\n",
        "\n",
        "        self.console.debug('checking if the mechanism is inf or zero...')\n",
        "        if np.isinf(eps) or self.is_inf() or self.is_zero():\n",
        "            self.update(noise_scale=0)\n",
        "            return 0.0\n",
        "        else:\n",
        "            self.console.debug('calibration begins...')\n",
        "            fn_err = lambda x: abs(eps - self.update(np.exp(x)).get_approxDP(delta))\n",
        "\n",
        "            # check if the mechanism is already calibrated\n",
        "            if fn_err(self.params['noise_scale']) < 1e-3:\n",
        "                return self.params['noise_scale']\n",
        "\n",
        "            result: OptimizeResult = minimize_scalar(fn_err,\n",
        "                method='brent',\n",
        "                options={'xtol': 1e-5, 'maxiter': 1000000, 'disp': 0}\n",
        "            )\n",
        "\n",
        "            if result.success and result.fun < 1e-3:\n",
        "                noise_scale = np.exp(result.x)\n",
        "                self.update(noise_scale)\n",
        "                return noise_scale\n",
        "            else:\n",
        "                raise RuntimeError(f\"calibrator failed to find noise scale\\n{result}\")\n",
        "\n",
        "\n",
        "\n",
        "# Implementing the NAP layer :\n",
        "class GaussianMechanism(NoisyMechanism):\n",
        "    def __init__(self, noise_scale: float):\n",
        "        # \"noise_scale\" is the std of the noise divided by the L2 sensitivity\n",
        "        super().__init__(noise_scale=noise_scale)\n",
        "        gm = ExactGaussianMechanism(sigma=noise_scale)\n",
        "        self.name = 'GaussianMechanism'\n",
        "        self.set_all_representation(gm)\n",
        "\n",
        "    def perturb(self, data: Tensor, sensitivity: float) -> Tensor:\n",
        "        std = self.params['noise_scale'] * sensitivity\n",
        "        return torch.normal(mean=data, std=std) if std else data\n",
        "\n",
        "    def __call__(self, data: Tensor, sensitivity: float) -> Tensor:\n",
        "        return self.perturb(data, sensitivity)\n",
        "\n",
        "\n",
        "\n",
        "class NAP(Module):\n",
        "    def __init__(self, noise_std: float, sensitivity: float) -> None:\n",
        "        super().__init__()\n",
        "        self.sensitivity = sensitivity\n",
        "        noise_scale = noise_std / sensitivity\n",
        "        self.mechanism = GaussianMechanism(noise_scale)\n",
        "\n",
        "    def forward(self, x: Tensor, adj_t: Tensor) -> Tensor:\n",
        "        x = F.normalize(x, p=2, dim=-1)                     # normalize\n",
        "        x = torch.spmm(adj_t, x)                            # aggregate\n",
        "        x = self.mechanism.perturb(x, self.sensitivity)     # perturb\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "whxpPqud70XP"
      },
      "outputs": [],
      "source": [
        "from autodp.transformer_zoo import ComposeGaussian, Composition\n",
        "#from autodp.mechanism_zoo import GaussianMechanism, ExactGaussianMechanism\n",
        "\n",
        "class ComposedNoisyMechanism(NoisyMechanism):\n",
        "    def __init__(self,\n",
        "                 noise_scale: float,\n",
        "                 mechanism_list: list[NoisyMechanism],\n",
        "                 coeff_list: list[float]=None,\n",
        "                 weight_list: list[float]=None,\n",
        "                 ):\n",
        "        super().__init__(noise_scale)\n",
        "        if coeff_list is None:\n",
        "            coeff_list = [1] * len(mechanism_list)\n",
        "        if weight_list is None:\n",
        "            weight_list = [1] * len(mechanism_list)\n",
        "        self.params = {\n",
        "            'noise_scale': noise_scale,\n",
        "            'mechanism_list': mechanism_list,\n",
        "            'coeff_list': coeff_list,\n",
        "            'weight_list': weight_list\n",
        "        }\n",
        "        mechanism_list = [mech.update(noise_scale = weight * noise_scale) for mech, weight in zip(mechanism_list, weight_list)]\n",
        "        all_gaussian = all(isinstance(mech, (GaussianMechanism, ExactGaussianMechanism)) for mech in mechanism_list)\n",
        "        CompositionCls = Composition\n",
        "        mech = CompositionCls()(mechanism_list, coeff_list)\n",
        "        self.set_all_representation(mech)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OEmb75rc8DNv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wlrZ7l-E8Dbp"
      },
      "outputs": [],
      "source": [
        "from torch.nn import ModuleList, Parameter\n",
        "\n",
        "\n",
        "class ProgressiveModule(TrainableModule):\n",
        "    def __init__(self, *,\n",
        "                 num_classes: int,\n",
        "                 num_stages: int,\n",
        "                 hidden_dim: int = 16,\n",
        "                 base_layers: int = 2,\n",
        "                 head_layers: int = 1,\n",
        "                 normalize: bool = True,\n",
        "                 jk_mode: str = 'cat',\n",
        "                 dropout: float = 0.0,\n",
        "                 activation_fn: Callable[[Tensor], Tensor] = torch.relu_,\n",
        "                 batch_norm: bool = True,\n",
        "                 layerwise: bool = False,\n",
        "                 **kwargs,\n",
        "                 ):\n",
        "\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_stages = num_stages\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.base_layers = base_layers\n",
        "        self.head_layers = head_layers\n",
        "        self.normalize = normalize\n",
        "        self.jk_mode = jk_mode\n",
        "        self.dropout = dropout\n",
        "        self.activation_fn = activation_fn\n",
        "        self.batch_norm = batch_norm\n",
        "        self.layerwise = layerwise\n",
        "\n",
        "        self.current_stage = 0\n",
        "\n",
        "        self.base = ModuleList(\n",
        "            MLP(\n",
        "                hidden_dim=hidden_dim,\n",
        "                output_dim=hidden_dim,\n",
        "                num_layers=base_layers,\n",
        "                activation_fn=activation_fn,\n",
        "                dropout=dropout,\n",
        "                batch_norm=batch_norm,\n",
        "                plain_last=False,\n",
        "            ) for _ in range(num_stages)\n",
        "        )\n",
        "\n",
        "        self.jk = ModuleList(\n",
        "            JumpingKnowledge(\n",
        "                mode=jk_mode,\n",
        "                hidden_dim=hidden_dim,\n",
        "                channels=hidden_dim,\n",
        "                num_layers=2,\n",
        "                num_heads=2\n",
        "            ) for _ in range(num_stages)\n",
        "        )\n",
        "\n",
        "        self.head = ModuleList(\n",
        "            MLP(\n",
        "                hidden_dim=hidden_dim,\n",
        "                output_dim=num_classes,\n",
        "                num_layers=head_layers,\n",
        "                dropout=dropout,\n",
        "                activation_fn=activation_fn,\n",
        "                batch_norm=batch_norm,\n",
        "                plain_last=True,\n",
        "            ) for _ in range(num_stages)\n",
        "        )\n",
        "\n",
        "    def set_stage(self, stage: int):\n",
        "        self.current_stage = stage\n",
        "        if self.layerwise:\n",
        "            # freeze previous layers\n",
        "            for i in range(self.current_stage):\n",
        "                for param in self.base[i].parameters():\n",
        "                    param.requires_grad = False\n",
        "                for param in self.jk[i].parameters():\n",
        "                    param.requires_grad = False\n",
        "                for param in self.head[i].parameters():\n",
        "                    param.requires_grad = False\n",
        "\n",
        "    def forward(self, xs: list[Tensor]) -> tuple[Tensor, Tensor]:\n",
        "        \"\"\"forward propagation\n",
        "\n",
        "        Args:\n",
        "            xs (list[Tensor]): list of aggregate node embeddings\n",
        "\n",
        "        Returns:\n",
        "            tuple[Tensor, Tensor]: node embeddings, node unnormalized predictions\n",
        "        \"\"\"\n",
        "\n",
        "        for i in range(self.current_stage + 1):\n",
        "            xs[i] = self.base[i](xs[i])\n",
        "\n",
        "        h = xs[-1]\n",
        "        x = self.jk[self.current_stage](torch.stack(xs, dim=-1))\n",
        "\n",
        "        if self.normalize:\n",
        "            x = F.normalize(x, p=2, dim=-1)\n",
        "\n",
        "        y = self.head[self.current_stage](x)\n",
        "        return h, y\n",
        "\n",
        "    def step(self, data: Data, phase: Phase) -> tuple[Optional[Tensor], Metrics]:\n",
        "        xs = [data[f'x{i}'][data.batch_nodes] for i in range(self.current_stage + 1)]\n",
        "        y = data.y[data.batch_nodes]\n",
        "\n",
        "        preds: Tensor = self(xs)[1]\n",
        "        acc = preds.detach().argmax(dim=1).eq(y).float().mean() * 100\n",
        "        metrics = {f'{phase}/acc': acc}\n",
        "\n",
        "        loss = None\n",
        "        if phase != 'test':\n",
        "            loss = F.cross_entropy(input=preds, target=y)\n",
        "            metrics[f'{phase}/loss'] = loss.detach()\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def predict(self, data: Data) -> tuple[Tensor, Tensor]:\n",
        "        xs = [data[f'x{i}'][data.batch_nodes] for i in range(self.current_stage + 1)]\n",
        "        x, y = self(xs)\n",
        "        return x, torch.softmax(y, dim=-1)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        self.current_stage = 0\n",
        "        for encoder in self.base:\n",
        "            encoder.reset_parameters()\n",
        "        for jk in self.jk:\n",
        "            jk.reset_parameters()\n",
        "        for head in self.head:\n",
        "            head.reset_parameters()\n",
        "        for param in super().parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def parameters(self, recurse: bool = True) -> Iterator[Parameter]:\n",
        "        if not self.layerwise:\n",
        "            for i in range(self.current_stage):\n",
        "                yield from self.base[i].parameters(recurse=recurse)\n",
        "        yield from self.base[self.current_stage].parameters(recurse=recurse)\n",
        "        yield from self.jk[self.current_stage].parameters(recurse=recurse)\n",
        "        yield from self.head[self.current_stage].parameters(recurse=recurse)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I34DCZ3R8D53"
      },
      "outputs": [],
      "source": [
        "class ProGAP (NodeClassification):\n",
        "    \"\"\"Non-private ProGAP method\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 depth:         Annotated[int,   ArgInfo(help='model depth', option='-k')] = 2,\n",
        "                 hidden_dim:    Annotated[int,   ArgInfo(help='dimension of the hidden layers')] = 16,\n",
        "                 base_layers:   Annotated[int,   ArgInfo(help='number of base MLP layers')] = 1,\n",
        "                 head_layers:   Annotated[int,   ArgInfo(help='number of head MLP layers')] = 1,\n",
        "                 jk:            Annotated[str,   ArgInfo(help='jumping knowledge combination scheme', choices=JumpingKnowledge.supported_modes)] = 'cat',\n",
        "                 activation:    Annotated[str,   ArgInfo(help='type of activation function', choices=['relu', 'selu', 'tanh'])] = 'selu',\n",
        "                 dropout:       Annotated[float, ArgInfo(help='dropout rate')] = 0.0,\n",
        "                 batch_norm:    Annotated[bool,  ArgInfo(help='if true, then model uses batch normalization')] = True,\n",
        "                 layerwise:     Annotated[bool,  ArgInfo(help='if true, then model uses layerwise training')] = False,\n",
        "                 optimizer:     Annotated[str,   ArgInfo(help='optimization algorithm', choices=['sgd', 'adam'])] = 'adam',\n",
        "                 learning_rate: Annotated[float, ArgInfo(help='learning rate', option='--lr')] = 0.01,\n",
        "                 weight_decay:  Annotated[float, ArgInfo(help='weight decay (L2 penalty)')] = 0.0,\n",
        "                 **kwargs:      Annotated[dict,  ArgInfo(help='extra options passed to base class', bases=[NodeClassification])]\n",
        "                 ):\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.depth = depth\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.base_layers = base_layers\n",
        "        self.head_layers = head_layers\n",
        "        self.jk = jk\n",
        "        self.activation = activation\n",
        "        self.dropout = dropout\n",
        "        self.batch_norm = batch_norm\n",
        "        self.layerwise = layerwise\n",
        "        self.optimizer = optimizer\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weight_decay = weight_decay\n",
        "        self.num_stages = depth + 1\n",
        "        self.nap = NAP(noise_std=0, sensitivity=1)\n",
        "        self.classifier: ProgressiveModule\n",
        "\n",
        "        self.console = Console()\n",
        "\n",
        "        super().__init__(num_classes, **kwargs)\n",
        "\n",
        "    def configure_classifier(self) -> ProgressiveModule:\n",
        "        return ProgressiveModule(\n",
        "            num_classes=self.num_classes,\n",
        "            num_stages=self.num_stages,\n",
        "            hidden_dim=self.hidden_dim,\n",
        "            base_layers=self.base_layers,\n",
        "            head_layers=self.head_layers,\n",
        "            normalize=True,\n",
        "            jk_mode=self.jk,\n",
        "            activation_fn=activation_resolver.make(self.activation),\n",
        "            dropout=self.dropout,\n",
        "            batch_norm=self.batch_norm,\n",
        "            layerwise=self.layerwise,\n",
        "            optimizer=self.optimizer,\n",
        "            learning_rate=self.learning_rate,\n",
        "            weight_decay=self.weight_decay,\n",
        "        )\n",
        "\n",
        "    def pipeline(self, fit: bool=False) -> Optional[Metrics]:\n",
        "        n = self.num_stages\n",
        "        self.data.x0 = self.data.x\n",
        "\n",
        "        for i in range(n):\n",
        "            if i > 0:\n",
        "                x, _ = self.trainer.predict(dataloader=self.data_loader('predict'))\n",
        "                x = self.nap(x, self.data.adj_t)\n",
        "                self.data[f'x{i}'] = x\n",
        "\n",
        "            self.classifier.set_stage(i)\n",
        "\n",
        "            if fit:\n",
        "                self.console.info(f'Fitting stage {i+1} of {n}')\n",
        "                self.trainer = self.configure_trainer()\n",
        "                metrics = super().fit()\n",
        "\n",
        "        self.data.ready = True\n",
        "        return metrics if fit else None\n",
        "\n",
        "    def fit(self) -> Metrics:\n",
        "        return self.pipeline(fit=True)\n",
        "\n",
        "    def test(self) -> Metrics:\n",
        "        \"\"\"Predict the labels for the given data, or the training data if data is None.\"\"\"\n",
        "        if not getattr(self.data, 'ready', False):\n",
        "            self.pipeline(fit=False)\n",
        "\n",
        "        return super().test()\n",
        "\n",
        "    def predict(self) -> torch.Tensor:\n",
        "        \"\"\"Predict the labels for the given data, or the training data if data is None.\"\"\"\n",
        "        if not getattr(self.data, 'ready', False):\n",
        "            self.pipeline(fit=False)\n",
        "\n",
        "        return super().predict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "EXQeUcJxjYuX",
        "outputId": "9c6db224-196c-4ec0-c191-4bbc9af6279a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠙</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:05:00</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.820</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.000</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.630</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.006</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.602</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠙\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:05:00\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m62.820\u001b[0m train/loss: \u001b[1;36m1.000\u001b[0m  val/acc: \u001b[1;36m62.630\u001b[0m val/loss: \u001b[1;36m1.006\u001b[0m  test/acc: \u001b[1;36m62.602\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 100, 'train/acc': tensor(62.8259), 'train/loss': tensor(0.9990), 'val/acc': tensor(62.6649), 'val/loss': tensor(1.0053), 'test/acc': tensor(62.6287)}\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the StandardGNN model with the desired parameters\n",
        "model = ProGAP(\n",
        "    num_classes=num_classes, # Assuming 'data.y' contains the class labels\n",
        "    hidden_dim=16, # Dimension of the hidden layers\n",
        "    base_layers=1, # Number of base MLP layers\n",
        "    head_layers=1, # Number of head MLP layers\n",
        "    activation='selu', # Activation function\n",
        "    dropout=0.0, # Dropout rate\n",
        "    batch_norm=True, # Batch normalization\n",
        "    jk='cat', # Jumping knowledge mode\n",
        "    optimizer='adam', # Optimization algorithm\n",
        "    learning_rate=0.01, # Learning rate\n",
        "    weight_decay=0.0 # Weight decay\n",
        ")\n",
        "\n",
        "# Instantiate the Logger\n",
        "# logger = Logger(\n",
        "#     logger='wandb', # Logger type\n",
        "#     project='ProGAP', # Project name for logger\n",
        "#     output_dir='./output', # Directory to store the results\n",
        "#     config={} # Additional configuration for the logger\n",
        "# )\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_pro = model.run(data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_pro)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv6LyMC7IpKM"
      },
      "source": [
        "#### For Reddit Data :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "MQ13g5ZeImsx",
        "outputId": "582a1f43-293d-4c95-88a5-99be27807d19"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠙</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:00:28</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">99.059</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.145</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.767</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.156</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.549</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠙\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:00:28\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m99.059\u001b[0m train/loss: \u001b[1;36m0.145\u001b[0m  val/acc: \u001b[1;36m98.767\u001b[0m val/loss: \u001b[1;36m0.156\u001b[0m  test/acc: \u001b[1;36m98.549\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'epoch': 99, 'train/acc': tensor(99.0595), 'train/loss': tensor(0.1445), 'val/acc': tensor(98.7670), 'val/loss': tensor(0.1560), 'test/acc': tensor(98.5488)}\n"
          ]
        }
      ],
      "source": [
        "# Instantiate the StandardGNN model with the desired parameters\n",
        "model = ProGAP(\n",
        "    num_classes=num_classes_reddit, # Assuming 'data.y' contains the class labels\n",
        "    hidden_dim=16, # Dimension of the hidden layers\n",
        "    base_layers=1, # Number of base MLP layers\n",
        "    head_layers=1, # Number of head MLP layers\n",
        "    activation='selu', # Activation function\n",
        "    dropout=0.0, # Dropout rate\n",
        "    batch_norm=True, # Batch normalization\n",
        "    jk='cat', # Jumping knowledge mode\n",
        "    optimizer='adam', # Optimization algorithm\n",
        "    learning_rate=0.01, # Learning rate\n",
        "    weight_decay=0.0 # Weight decay\n",
        ")\n",
        "\n",
        "# Instantiate the Logger\n",
        "# logger = Logger(\n",
        "#     logger='wandb', # Logger type\n",
        "#     project='ProGAP', # Project name for logger\n",
        "#     output_dir='./output', # Directory to store the results\n",
        "#     config={} # Additional configuration for the logger\n",
        "# )\n",
        "\n",
        "# Train the model on the data\n",
        "metrics_pro_reddit = model.run(reddit_data, fit=True, test=True)\n",
        "\n",
        "# Print the metrics\n",
        "print(metrics_pro_reddit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "reutqmIp8riX"
      },
      "outputs": [],
      "source": [
        "class EdgeLevelProGAP (ProGAP):\n",
        "    \"\"\"Edge-level private ProGAP method\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 num_classes,\n",
        "                 epsilon:       Annotated[float, ArgInfo(help='DP epsilon parameter', option='-e')],\n",
        "                 delta:         Annotated[Union[Literal['auto'], float],\n",
        "                                                 ArgInfo(help='DP delta parameter (if \"auto\", sets a proper value based on data size)', option='-d')] = 'auto',\n",
        "                 **kwargs:      Annotated[dict,  ArgInfo(help='extra options passed to base class', bases=[ProGAP])]\n",
        "                 ):\n",
        "\n",
        "        super().__init__(num_classes, **kwargs)\n",
        "        self.epsilon = epsilon\n",
        "        self.delta = delta\n",
        "        self.num_edges = None  # will be used to set delta if it is 'auto'\n",
        "        # Noise std of NAP is set to 0, and will be calibrated later\n",
        "        self.nap = NAP(noise_std=0, sensitivity=1)\n",
        "        self.console = Console()\n",
        "    def calibrate(self):\n",
        "        with self.console.status('calibrating noise to privacy budget'):\n",
        "            if self.delta == 'auto':\n",
        "                self.console.info('num_edges = %d' % self.num_edges)\n",
        "                delta = 0.0 if np.isinf(self.epsilon) else 1. / (10 ** len(str(self.num_edges)))\n",
        "                self.console.info('delta = %.0e' % delta)\n",
        "\n",
        "            if np.isinf(self.epsilon):\n",
        "                self.noise_scale = 0.0\n",
        "            else:\n",
        "                mechanism_list = [self.nap.mechanism]\n",
        "                # Check if all mechanisms are instances of GaussianMechanism or ExactGaussianMechanism\n",
        "                if all(isinstance(mech, (GaussianMechanism, ExactGaussianMechanism)) for mech in mechanism_list):\n",
        "                    composed_mechanism = ComposedNoisyMechanism(\n",
        "                        noise_scale=1.0,\n",
        "                        mechanism_list=mechanism_list,\n",
        "                    )\n",
        "\n",
        "                else:\n",
        "                    print(\"All mechanisms must be instances of GaussianMechanism or ExactGaussianMechanism.\")\n",
        "                self.noise_scale = composed_mechanism.calibrate(eps=self.epsilon, delta=delta)\n",
        "\n",
        "            self.console.info(f'noise scale: {self.noise_scale:.4f}\\n')\n",
        "\n",
        "    def setup(self, data: Data) -> None:\n",
        "        super().setup(data)\n",
        "        m = self.data.num_edges\n",
        "        if self.num_edges != m:\n",
        "            self.num_edges = m\n",
        "            self.calibrate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ufhw68L2Pv9s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "fTl0lKvfPS5J",
        "outputId": "8417600d-f18f-4132-e4cd-fdcf03228a91"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠸</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:05:40</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.454</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.998</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.196</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.005</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">62.201</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠸\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:05:40\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m62.454\u001b[0m train/loss: \u001b[1;36m0.998\u001b[0m  val/acc: \u001b[1;36m62.196\u001b[0m val/loss: \u001b[1;36m1.005\u001b[0m  test/acc: \u001b[1;36m62.201\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epsilon_values = [0.1, 0.5, 1, 2, 5, 10] # Example epsilon values\n",
        "delta_value = 'auto'\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    model = EdgeLevelProGAP(num_classes=num_classes, epsilon=epsilon, delta=delta_value)\n",
        "    model.setup(data)\n",
        "    model.fit()\n",
        "    metrics = model.test()\n",
        "    metrics_list.append(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lNPg_9LpPX9S",
        "outputId": "2b3a8fe7-150b-4d02-88f9-26e986ccfecd"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming metrics_list contains dictionaries with 'accuracy' key\n",
        "accuracy_values = [metrics['test/acc'] for metrics in metrics_list]\n",
        "baseline_acc = [metrics_base_mlp['test/acc'] for metrics in metrics_list]\n",
        "baseline_gnn_acc = [metrics_base_gnn['test/acc'] for metrics in metrics_list]\n",
        "# Plotting\n",
        "plt.plot(epsilon_values, accuracy_values, marker='o', label = 'Our Differentially Private Model')\n",
        "plt.plot(epsilon_values, baseline_acc, marker = 'x', label = 'Baseline MLP')\n",
        "plt.plot(epsilon_values, baseline_gnn_acc, marker = 'x', label = 'Baseline GNN' )\n",
        "plt.legend()\n",
        "plt.title('Privacy Cost vs Inference Tradeoff : FB-100')\n",
        "plt.xlabel('Epsilon Values')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "vbOx-bTH8-dK",
        "outputId": "2ecca829-9143-4b99-8f7c-16e61936bf74"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠴</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:00:39</span>           \n",
              "                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.968</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.146</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.810</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.152</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">98.503</span>         \n",
              "                                                                                                                   \n",
              "</pre>\n"
            ],
            "text/plain": [
              "                  \u001b[32m⠴\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:00:39\u001b[0m           \n",
              "                    train/acc: \u001b[1;36m98.968\u001b[0m train/loss: \u001b[1;36m0.146\u001b[0m  val/acc: \u001b[1;36m98.810\u001b[0m val/loss: \u001b[1;36m0.152\u001b[0m  test/acc: \u001b[1;36m98.503\u001b[0m         \n",
              "                                                                                                                   \n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ],
            "text/plain": []
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "epsilon_values = [0.1, 0.5, 1, 2, 5, 10] # Example epsilon values\n",
        "delta_value = 'auto'\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "for epsilon in epsilon_values:\n",
        "    model = EdgeLevelProGAP(num_classes=num_classes_reddit, epsilon=epsilon, delta=delta_value)\n",
        "    model.setup(reddit_data)\n",
        "    model.fit()\n",
        "    metrics = model.test()\n",
        "    metrics_list.append(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dc-jGqb1-mmQ",
        "outputId": "9e8e3ac6-ee5a-4ff8-84b2-1d0118690714"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: autodp\n",
            "Version: 0.2.3.1\n",
            "Summary: Automating Differential Privacy Computation\n",
            "Home-page: https://github.com/yuxiangw/autodp\n",
            "Author: Yu-Xiang Wang\n",
            "Author-email: yuxiangw@cs.ucsb.edu\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: absl-py, matplotlib, numpy, scipy\n",
            "Required-by: \n"
          ]
        }
      ],
      "source": [
        "!pip show autodp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XB21H1bsqrIn",
        "outputId": "3cea1601-d974-4853-8c8f-5c80df847a08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(93.2869)\n"
          ]
        }
      ],
      "source": [
        "print(metrics_list[0]['test/acc'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "h24FG045B-mD"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # Assuming metrics_list contains dictionaries with 'accuracy' key\n",
        "# accuracy_values = [metrics['test/acc'] for metrics in metrics_list]\n",
        "# baseline_acc = [metrics_base_mlp_reddit['test/acc'] for metrics in metrics_list]\n",
        "# baseline_gnn_acc = [metrics_pro_reddit['test/acc'] for metrics in metrics_list]\n",
        "# # Plotting\n",
        "# plt.plot(epsilon_values, accuracy_values, marker='o')\n",
        "# plt.title('Privacy Cost vs Inference Tradeoff : Reddit Data')\n",
        "# plt.xlabel('Epsilon Values')\n",
        "# plt.ylabel('Test Accuracy')\n",
        "# plt.grid(True)\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eM82C2GQCn2j",
        "outputId": "86f36798-fefa-46f1-c054-c408a27cb76e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(82.2602)"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "metrics_base_mlp_reddit['test/acc']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "R6A_O6VDecBj",
        "outputId": "ce149311-841e-4f01-b901-659c0c192679"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming metrics_list contains dictionaries with 'accuracy' key\n",
        "accuracy_values = [metrics['test/acc'] for metrics in metrics_list]\n",
        "baseline_acc = [metrics_base_mlp_reddit['test/acc'] for metrics in metrics_list]\n",
        "baseline_gnn_acc = [metrics_pro_reddit['test/acc'] for metrics in metrics_list]\n",
        "# Plotting\n",
        "plt.plot(epsilon_values, accuracy_values, marker='o', label = 'Our Differentially Private Model')\n",
        "plt.plot(epsilon_values, baseline_acc, marker = 'x', label = 'Baseline MLP')\n",
        "plt.plot(epsilon_values, baseline_gnn_acc, marker = 'x', label = 'Baseline GNN' )\n",
        "plt.legend()\n",
        "plt.title('Privacy Cost vs Inference Tradeoff : Reddit Data')\n",
        "plt.xlabel('Epsilon Values')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "83fZQ5AtfBlB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "1099b26d1696436a9a48017562acad8d": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_d093526d7989486d9500b32fb6bf6ad8",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠹</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:00:21</span>           \n                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84.148</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.472</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">81.954</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.550</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">82.220</span>         \n                                                                                                                   \n</pre>\n",
                  "text/plain": "                  \u001b[32m⠹\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:00:21\u001b[0m           \n                    train/acc: \u001b[1;36m84.148\u001b[0m train/loss: \u001b[1;36m0.472\u001b[0m  val/acc: \u001b[1;36m81.954\u001b[0m val/loss: \u001b[1;36m0.550\u001b[0m  test/acc: \u001b[1;36m82.220\u001b[0m         \n                                                                                                                   \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        },
        "7de337c84a3640a68c3c50c859b8a780": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d093526d7989486d9500b32fb6bf6ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e629fa88b5f043a6aa6d935d5da48ba8": {
          "model_module": "@jupyter-widgets/output",
          "model_module_version": "1.0.0",
          "model_name": "OutputModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_7de337c84a3640a68c3c50c859b8a780",
            "msg_id": "",
            "outputs": [
              {
                "data": {
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                  <span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> overal progress <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>/<span style=\"color: #008080; text-decoration-color: #008080\">100</span> epochs <span style=\"color: #f92672; text-decoration-color: #f92672\">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╸</span> <span style=\"color: #008080; text-decoration-color: #008080\"> 99</span>% <span style=\"color: #808000; text-decoration-color: #808000\">0:02:29</span>           \n                    train/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.522</span> train/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.475</span>  val/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">34.563</span> val/loss: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.482</span>  test/acc: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">35.021</span>         \n                                                                                                                   \n</pre>\n",
                  "text/plain": "                  \u001b[32m⠼\u001b[0m overal progress \u001b[36m 99\u001b[0m/\u001b[36m100\u001b[0m epochs \u001b[38;2;249;38;114m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[38;2;249;38;114m╸\u001b[0m \u001b[36m 99\u001b[0m% \u001b[33m0:02:29\u001b[0m           \n                    train/acc: \u001b[1;36m35.522\u001b[0m train/loss: \u001b[1;36m1.475\u001b[0m  val/acc: \u001b[1;36m34.563\u001b[0m val/loss: \u001b[1;36m1.482\u001b[0m  test/acc: \u001b[1;36m35.021\u001b[0m         \n                                                                                                                   \n"
                },
                "metadata": {},
                "output_type": "display_data"
              }
            ]
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}